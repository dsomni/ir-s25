{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6656b951",
   "metadata": {},
   "outputs": [],
   "source": [
    "from g4f.client import Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5575255",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nikit\\PycharmProjects\\ir-s25\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "from sklearn.neighbors import BallTree\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "from utils import from_current_file, load\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b97b0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTEmbedder:\n",
    "    def __init__(\n",
    "        self, model_name=\"sentence-transformers/msmarco-bert-base-dot-v5\", device=DEVICE\n",
    "    ):\n",
    "        self.device = device or \"cpu\"\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModel.from_pretrained(model_name).to(self.device)\n",
    "        self.model.eval()\n",
    "\n",
    "        self.do_lower_case = getattr(self.tokenizer, \"do_lower_case\", False)\n",
    "\n",
    "    def text_to_embedding(self, texts, pooling=\"mean\", normalize=False):\n",
    "        is_single = isinstance(texts, str)\n",
    "        texts = [texts] if is_single else texts\n",
    "\n",
    "        inputs = self.tokenizer(\n",
    "            texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=512\n",
    "        ).to(self.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "\n",
    "        if pooling == \"mean\":\n",
    "            mask = inputs[\"attention_mask\"].unsqueeze(-1)\n",
    "            embeddings = (outputs.last_hidden_state * mask).sum(1) / mask.sum(1).clamp(\n",
    "                min=1e-9\n",
    "            )\n",
    "        elif pooling == \"cls\":\n",
    "            embeddings = outputs.last_hidden_state[:, 0, :]\n",
    "        else:\n",
    "            raise ValueError(\"Invalid pooling method\")\n",
    "\n",
    "        if normalize:\n",
    "            embeddings = torch.nn.functional.normalize(embeddings, p=2, dim=1)\n",
    "\n",
    "        return embeddings.cpu().numpy()[0] if is_single else embeddings.cpu().numpy()\n",
    "\n",
    "\n",
    "def batch(iterable, batch_size):\n",
    "    all_batches = []\n",
    "    current_batch = []\n",
    "\n",
    "    for item in iterable:\n",
    "        current_batch.append(item)\n",
    "        if len(current_batch) == batch_size:\n",
    "            all_batches.append(current_batch)\n",
    "            current_batch = []\n",
    "\n",
    "    if current_batch:  # Add the last partial batch\n",
    "        all_batches.append(current_batch)\n",
    "\n",
    "    return all_batches\n",
    "\n",
    "\n",
    "class BERTBallTree:\n",
    "    def __init__(\n",
    "        self,\n",
    "        index_dir: str = \"../data/embedding_directory\",\n",
    "        documents_dir: str = \"../data/scrapped/class_data_function__1_1\",\n",
    "        top_similar: int = 10,\n",
    "        force: bool = False,\n",
    "        embedder=None,\n",
    "        metric=\"euclidean\",\n",
    "        leaf_size=1,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the BERT Ball Tree.\n",
    "\n",
    "        Args:\n",
    "            embedder: Pre-initialized BERTEmbedder instance\n",
    "            metric: Distance metric for BallTree ('euclidean', 'cosine', etc.)\n",
    "            leaf_size: Affects the speed of queries and memory usage\n",
    "        \"\"\"\n",
    "        self._index_dir = from_current_file(index_dir)\n",
    "        self._documents_dir = from_current_file(documents_dir)\n",
    "        self.top_similar = top_similar\n",
    "        self.embedder = embedder or BERTEmbedder()\n",
    "        self.metric = metric\n",
    "        self.leaf_size = leaf_size\n",
    "        self.tree = None\n",
    "        self.texts = None\n",
    "        self.documents: dict[int, str] = {}\n",
    "\n",
    "    def build_tree(self):\n",
    "        \"\"\"\n",
    "        Build the Ball Tree from a list of texts.\n",
    "\n",
    "        Args:\n",
    "            texts: List of strings to index\n",
    "        \"\"\"\n",
    "        sentences = []\n",
    "        for document_id, filename in enumerate(os.listdir(self._documents_dir)):\n",
    "            if filename.endswith(\".txt\"):\n",
    "                with open(\n",
    "                    os.path.join(self._documents_dir, filename), \"r\", encoding=\"utf-8\"\n",
    "                ) as f:\n",
    "                    text = f.read()\n",
    "                    self.documents[document_id] = filename[:-4]\n",
    "                    sentences.append(text)\n",
    "\n",
    "        embeddings = []\n",
    "        for b in tqdm(batch(sentences, 64)):\n",
    "            batch_embeddings = self.embedder.text_to_embedding(\n",
    "                b, pooling=\"mean\", normalize=True\n",
    "            )\n",
    "            embeddings.extend(batch_embeddings)\n",
    "        print(len(embeddings))\n",
    "        self.tree = BallTree(embeddings, metric=self.metric, leaf_size=self.leaf_size)\n",
    "\n",
    "    def query(self, query_text, k=5, return_distances=False):\n",
    "        \"\"\"\n",
    "        Query the Ball Tree for nearest neighbors.\n",
    "\n",
    "        Args:\n",
    "            query_text: The query text string\n",
    "            k: Number of nearest neighbors to return\n",
    "            return_distances: Whether to return distances along with results\n",
    "\n",
    "        Returns:\n",
    "            If return_distances is False: list of nearest texts\n",
    "            If return_distances is True: tuple of (texts, distances)\n",
    "        \"\"\"\n",
    "        if self.tree is None:\n",
    "            raise ValueError(\"Ball Tree has not been built yet. Call build_tree() first.\")\n",
    "\n",
    "        # Get embedding for the query text\n",
    "        query_embedding = self.embedder.text_to_embedding(\n",
    "            query_text, pooling=\"mean\", normalize=True\n",
    "        ).reshape(1, -1)\n",
    "\n",
    "        # Query the tree\n",
    "        distances, indices = self.tree.query(query_embedding, k=k)\n",
    "\n",
    "        # Get the corresponding texts\n",
    "        print(indices[0])\n",
    "        results = []\n",
    "        for indice in indices[0]:\n",
    "            results.append(self.documents[indice])\n",
    "\n",
    "        if return_distances:\n",
    "            return indices[0], results, distances\n",
    "        return results\n",
    "\n",
    "    def save_tree(self, filepath):\n",
    "        \"\"\"Save the Ball Tree and associated data to disk.\"\"\"\n",
    "        import joblib\n",
    "\n",
    "        data = {\n",
    "            \"texts\": self.texts,\n",
    "            \"tree\": self.tree,\n",
    "            \"metric\": self.metric,\n",
    "            \"leaf_size\": self.leaf_size,\n",
    "        }\n",
    "        joblib.dump(data, filepath)\n",
    "\n",
    "    @classmethod\n",
    "    def load_tree(cls, filepath, embedder=None):\n",
    "        \"\"\"Load a saved Ball Tree from disk.\"\"\"\n",
    "        import joblib\n",
    "\n",
    "        data = joblib.load(filepath)\n",
    "        instance = cls(\n",
    "            embedder=embedder, metric=data[\"metric\"], leaf_size=data[\"leaf_size\"]\n",
    "        )\n",
    "        instance.texts = data[\"texts\"]\n",
    "        instance.tree = data[\"tree\"]\n",
    "        return instance\n",
    "\n",
    "\n",
    "# Initialize and build the tree\n",
    "ball_tree = BERTBallTree()\n",
    "ball_tree.build_tree()\n",
    "\n",
    "# Query the tree\n",
    "# query = \"sin\"\n",
    "# results, distances = ball_tree.query(query, k=10, return_distances=True)\n",
    "\n",
    "# print(f\"Query: {query}\")\n",
    "# print(\"Top 3 results:\")\n",
    "# for text, dist in zip(results, distances):\n",
    "#     print(f\"- {text} (distance: {dist:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2263ba",
   "metadata": {},
   "source": [
    "### ADD INDEXER/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed98b7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAG:\n",
    "    folder_path = \"../data/scrapped/class_data_function__1_1\"\n",
    "\n",
    "    def __init__(self, model: str):\n",
    "        self.client = Client()\n",
    "\n",
    "    def _retrive_docs(self, query_m: str, k: int = 5):\n",
    "        index, results, _ = ball_tree.query(query_m, k)\n",
    "\n",
    "        contents = []\n",
    "\n",
    "        for name in results:\n",
    "            content = load(os.path.join(self.folder_path, name))\n",
    "\n",
    "            contents.append(content)\n",
    "\n",
    "        return contents\n",
    "\n",
    "    def get_answer(self, question: str, model: str, k: int = 5):\n",
    "        self.model = model\n",
    "\n",
    "        context = self._retrive_docs(question)\n",
    "\n",
    "        prompt = (\n",
    "            f\"You're a Python expert. Answer strictly according to the documentation wich is marked as 'Context' below. \"\n",
    "            'If there is no answer in the context, say, \"I can\\'t find the answer in the Python documentation\"\\n'\n",
    "            \"\\nContext:\\n\"\n",
    "            f\"{'\\n\\n'.join([f'{idx + 1}. {c}' for idx, c in enumerate(context)])}\\n\"\n",
    "            \"\\nQuestion: \\n\"\n",
    "            \"\\nResponse (with reference to the source [1-{2}]):\\n\"\n",
    "        )\n",
    "\n",
    "        messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=self.model, messages=messages, web_search=False\n",
    "        )\n",
    "\n",
    "        return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3367be08",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_model = RAG()\n",
    "\n",
    "question = \"Sin and Cos\"\n",
    "\n",
    "rag_model.get_answer(question=question, model=\"gpt4gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17cb784d",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = [\"fskdfj\", \"fs;ldgkjlk\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1e251f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "75269e3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You're a Python expert. Answer strictly according to the documentation wich is marked as 'Context' below. If there is no answer in the context, say, \"I can't find the answer in the Python documentation\"\n",
      "\n",
      "Context:\n",
      "1. fskdfj\n",
      "\n",
      "2. fs;ldgkjlk\n",
      "\n",
      "Question: \n",
      "\n",
      "Response (with reference to the source [1-{2}]):\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16de711c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
