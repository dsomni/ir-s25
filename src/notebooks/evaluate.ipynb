{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import logging\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "\n",
    "import nest_asyncio\n",
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.translate.bleu_score import SmoothingFunction, sentence_bleu\n",
    "from rouge import Rouge\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm import tqdm\n",
    "\n",
    "ROOT_FOLDERS = [os.path.join(\".\", \"..\"), os.path.join(\".\", \"..\", \"..\")]\n",
    "for folder in ROOT_FOLDERS:\n",
    "    if folder not in sys.path:\n",
    "        sys.path.insert(0, folder)\n",
    "\n",
    "from src.pipeline import ApiModel, Indexer, IndexerPipeline, RAGPipeline\n",
    "from src.utils import from_current_file, load, load_json, remove_path, save_json\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "logging.disable(logging.WARNING)\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = from_current_file(\"../data/scrapped/class_data_function__1_1\")\n",
    "\n",
    "NAME_TO_IDX = {\n",
    "    filename[:-4]: document_id\n",
    "    for document_id, filename in enumerate(os.listdir(DATA_PATH))\n",
    "}\n",
    "IDX_TO_NAME = {v: k for k, v in NAME_TO_IDX.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "INDEXER_PIPELINE = IndexerPipeline()\n",
    "RAG_PIPELINE = RAGPipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "LLM_RES_PATH = from_current_file(\"../data/evaluation/llm_responses.json\")\n",
    "QUERIES_PATH = from_current_file(\"../data/evaluation/queries.json\")\n",
    "INDEXER_RES_PATH = from_current_file(\"../data/evaluation/indexer_responses.json\")\n",
    "\n",
    "LLM_METRICS_PATH = from_current_file(\"../data/evaluation/llm_metrics.json\")\n",
    "GENERAl_METRICS_PATH = from_current_file(\"../data/evaluation/general_metrics.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_queries(\n",
    "    path: Path = QUERIES_PATH,\n",
    ") -> list[tuple[str, str, list[str]]]:\n",
    "    return [(k, v[\"query\"], v[\"ground_truths\"]) for k, v in load_json(path).items()]\n",
    "\n",
    "\n",
    "queries = load_queries()\n",
    "# queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:00<00:00, 182361.04it/s]\n"
     ]
    }
   ],
   "source": [
    "def check_validity():\n",
    "    queries = load_queries()\n",
    "\n",
    "    for _, __, ground_truths in tqdm(queries):\n",
    "        for gt in ground_truths:\n",
    "            if gt not in NAME_TO_IDX:\n",
    "                raise RuntimeError(f\"Invalid ground truth: {gt}\")\n",
    "\n",
    "\n",
    "check_validity()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/24 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:20<00:00,  1.18it/s]\n"
     ]
    }
   ],
   "source": [
    "def generate_indexer_responses(\n",
    "    indexer_res_path: Path = INDEXER_RES_PATH,\n",
    "    force: bool = False,\n",
    ") -> dict:\n",
    "    queries = load_queries()\n",
    "\n",
    "    indexers = INDEXER_PIPELINE.available_indexers\n",
    "    if force:\n",
    "        remove_path(indexer_res_path)\n",
    "    indexer_responses = load_json(indexer_res_path, allow_empty=True)\n",
    "\n",
    "    for query_name, query, ground_truths in tqdm(queries):\n",
    "        for indexer in indexers:\n",
    "            if indexer not in indexer_responses:\n",
    "                indexer_responses[indexer] = {}\n",
    "            if query_name in indexer_responses[indexer]:\n",
    "                continue\n",
    "            _, docs_scores = INDEXER_PIPELINE.index(query, indexer, k=10)\n",
    "            docs = [doc for doc, _ in docs_scores]\n",
    "            indexer_responses[indexer][query_name] = {\n",
    "                \"query\": query_name,\n",
    "                \"responses\": docs,\n",
    "                \"responses_idx\": [NAME_TO_IDX[d] for d in docs],\n",
    "                \"ground_truths\": ground_truths,\n",
    "                \"ground_truths_idx\": [NAME_TO_IDX[d] for d in ground_truths],\n",
    "            }\n",
    "    save_json(indexer_res_path, indexer_responses)\n",
    "    return indexer_responses\n",
    "\n",
    "\n",
    "indexer_responses = generate_indexer_responses()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_from_response(text: str) -> list[int]:\n",
    "    try:\n",
    "        # Find all matches\n",
    "        ref_matches = re.findall(r\"\\[(\\d+(?:,\\s*\\d+)*)\\]\", text.split(\"References:\")[1])\n",
    "        # ref_matches = re.findall(pattern, text)\n",
    "\n",
    "        # Extract individual numbers\n",
    "        numbers = []\n",
    "        for match in ref_matches:\n",
    "            numbers.extend([num.strip() for num in match.split(\",\")])\n",
    "\n",
    "        # Convert to integers\n",
    "        numbers = list(map(int, numbers))\n",
    "        return numbers\n",
    "    except BaseException:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def access_llm(\n",
    "    query: str, model: ApiModel, indexer: Indexer, model_name: str, k=10\n",
    ") -> tuple[str, list[str], str, Optional[str]]:\n",
    "    try:\n",
    "        response, sources = await RAG_PIPELINE.request_async(\n",
    "            query, model, k=k, indexer=indexer\n",
    "        )\n",
    "        return response, sources, model_name, None\n",
    "    except Exception as e:\n",
    "        return \"\", [], model_name, str(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/24 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 14/24 [02:41<01:53, 11.38s/it]Unclosed client session\n",
      "client_session: <g4f.requests.aiohttp.StreamSession object at 0x7fc9c0621e80>\n",
      "Unclosed connector\n",
      "connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x7fc9e4119790>, 11931.143030374)])']\n",
      "connector: <aiohttp.connector.TCPConnector object at 0x7fc9c06200e0>\n",
      "100%|██████████| 24/24 [04:23<00:00, 10.98s/it]\n"
     ]
    }
   ],
   "source": [
    "async def generate_llm_responses(\n",
    "    models: list[ApiModel],\n",
    "    timeout: float = 60.0,\n",
    "    llm_res_path: Path = LLM_RES_PATH,\n",
    "    force: bool = False,\n",
    ") -> dict:\n",
    "    queries = load_queries()\n",
    "\n",
    "    indexers = INDEXER_PIPELINE.available_indexers\n",
    "    if force:\n",
    "        remove_path(llm_res_path)\n",
    "\n",
    "    llm_responses = load_json(llm_res_path, allow_empty=True)\n",
    "\n",
    "    for query_name, query, ground_truths in tqdm(queries):\n",
    "        results = []\n",
    "        for model in models:\n",
    "            for indexer in indexers:\n",
    "                model_name = f\"{model} + {indexer}\"\n",
    "                if model_name not in llm_responses:\n",
    "                    llm_responses[model_name] = {}\n",
    "                if query_name in llm_responses[model_name]:\n",
    "                    continue\n",
    "\n",
    "                results.append(access_llm(query, model, indexer, model_name))\n",
    "\n",
    "        for coro in asyncio.as_completed(results):\n",
    "            try:\n",
    "                llm_res, sources, model_name, err = await asyncio.wait_for(coro, timeout)\n",
    "                if err is not None:\n",
    "                    continue\n",
    "                local_indices = extract_from_response(llm_res)\n",
    "                docs = [sources[idx - 1] for idx in local_indices]\n",
    "                llm_responses[model_name][query_name] = {\n",
    "                    \"query\": query_name,\n",
    "                    \"llm_output\": llm_res,\n",
    "                    \"sources\": sources,\n",
    "                    \"sources_idx\": [NAME_TO_IDX[d] for d in sources],\n",
    "                    \"responses\": docs,\n",
    "                    \"responses_idx\": [NAME_TO_IDX[d] for d in docs],\n",
    "                    \"ground_truths\": ground_truths,\n",
    "                    \"ground_truths_idx\": [NAME_TO_IDX[d] for d in ground_truths],\n",
    "                }\n",
    "            except asyncio.TimeoutError:\n",
    "                continue\n",
    "            except BaseException:\n",
    "                continue\n",
    "        await asyncio.sleep(2)\n",
    "\n",
    "    save_json(llm_res_path, llm_responses)\n",
    "    return llm_responses\n",
    "\n",
    "\n",
    "llm_responses = await generate_llm_responses([\"evil\", \"qwen-2-72b\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_sim(a, b):\n",
    "    return cosine_similarity([a], [b])[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_context(doc_names: list[str]) -> list[str]:\n",
    "    return [load(os.path.join(DATA_PATH, f\"{doc_name}.txt\")) for doc_name in doc_names]\n",
    "\n",
    "\n",
    "def compute_query_llm_metrics(query_result: dict, get_embedding):\n",
    "    rouge = Rouge()\n",
    "    smooth = SmoothingFunction().method1\n",
    "\n",
    "    query = query_result[\"query\"]\n",
    "    llm_output = query_result[\"llm_output\"]\n",
    "    contexts = build_context(query_result[\"responses\"])\n",
    "    ground_truths = \"\\n\".join(query_result[\"ground_truths\"])\n",
    "\n",
    "    emb_query = get_embedding(query)\n",
    "    emb_llm_output = get_embedding(llm_output)\n",
    "    emb_ground_truths = get_embedding(ground_truths)\n",
    "    emb_contexts = [get_embedding(c) for c in contexts]\n",
    "    emb_context_all = get_embedding(\" \".join(contexts))\n",
    "\n",
    "    # 1. faithfulness: answer vs context\n",
    "    faithfulness_score = cosine_sim(emb_llm_output, emb_context_all)\n",
    "\n",
    "    # 2. answer relevancy: answer vs question\n",
    "    answer_relevancy_score = cosine_sim(emb_llm_output, emb_query)\n",
    "\n",
    "    # 3. context recall: ground truth vs context\n",
    "    context_recall_score = cosine_sim(emb_ground_truths, emb_context_all)\n",
    "\n",
    "    # 4. context precision: answer vs each context (max similarity)\n",
    "    precisions = [cosine_sim(emb_llm_output, ctx_emb) for ctx_emb in emb_contexts]\n",
    "    context_precision_score = np.max(precisions) if precisions else 0.0\n",
    "\n",
    "    # 5. BLEU: answer vs ground truth\n",
    "    bleu_score = sentence_bleu(\n",
    "        [nltk.word_tokenize(ground_truths.lower())],\n",
    "        nltk.word_tokenize(llm_output.lower()),\n",
    "        smoothing_function=smooth,\n",
    "        weights=(0.5, 0.5),  # BLEU-2\n",
    "    )\n",
    "\n",
    "    # 6. ROUGE: answer vs ground truth\n",
    "    rouge_scores = rouge.get_scores(llm_output, ground_truths)[0]\n",
    "    rouge1 = rouge_scores[\"rouge-1\"][\"f\"]\n",
    "    rougeL = rouge_scores[\"rouge-l\"][\"f\"]\n",
    "\n",
    "    return {\n",
    "        \"faithfulness\": round(faithfulness_score, 3),\n",
    "        \"answer_relevancy\": round(answer_relevancy_score, 3),\n",
    "        \"context_recall\": round(context_recall_score, 3),\n",
    "        \"context_precision\": round(context_precision_score, 3),\n",
    "        \"bleu\": round(bleu_score, 3),\n",
    "        \"rouge1\": round(rouge1, 3),\n",
    "        \"rougeL\": round(rougeL, 3),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "evil + inverted_idx: 100%|██████████| 24/24 [00:16<00:00,  1.46it/s]\n",
      "evil + llm_tree_idx: 100%|██████████| 24/24 [00:32<00:00,  1.36s/it]\n",
      "qwen-2-72b + inverted_idx: 100%|██████████| 24/24 [00:17<00:00,  1.35it/s]\n",
      "qwen-2-72b + llm_tree_idx: 100%|██████████| 24/24 [00:20<00:00,  1.14it/s]\n"
     ]
    }
   ],
   "source": [
    "def compute_llm_metrics(\n",
    "    llm_metrics_path: Path = LLM_METRICS_PATH,\n",
    "    llm_res_path: Path = LLM_RES_PATH,\n",
    "    force: bool = False,\n",
    ") -> dict:\n",
    "    model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "    llm_responses = load_json(llm_res_path)\n",
    "\n",
    "    if force:\n",
    "        remove_path(llm_metrics_path)\n",
    "\n",
    "    llm_metrics = load_json(llm_metrics_path, allow_empty=True)\n",
    "\n",
    "    for model_name, queries in llm_responses.items():\n",
    "        for query_name, query_res in tqdm(queries.items(), desc=model_name):\n",
    "            if model_name not in llm_metrics:\n",
    "                llm_metrics[model_name] = {}\n",
    "            if query_name in llm_metrics[model_name]:\n",
    "                continue\n",
    "            llm_metrics[model_name][query_name] = compute_query_llm_metrics(\n",
    "                query_res, get_embedding=lambda x: model.encode([x])[0]\n",
    "            )\n",
    "        # Compute mean\n",
    "        llm_metrics[model_name][\"mean\"] = {\n",
    "            k: np.mean(\n",
    "                [v[k] for v in llm_metrics[model_name].values() if isinstance(v, dict)]\n",
    "            )\n",
    "            for k in list(llm_metrics[model_name].values())[0].keys()\n",
    "        }\n",
    "\n",
    "    save_json(llm_metrics_path, llm_metrics)\n",
    "\n",
    "    return llm_metrics\n",
    "\n",
    "\n",
    "general_metrics = compute_llm_metrics(force=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_at_k(ground_truth, predicted, k):\n",
    "    \"\"\"Compute Precision@k\"\"\"\n",
    "    if k == 0:\n",
    "        return 0.0\n",
    "    predicted = predicted[:k]\n",
    "    relevant = set(ground_truth)\n",
    "    hits = sum(1 for item in predicted if item in relevant)\n",
    "    return hits / k\n",
    "\n",
    "\n",
    "def recall_at_k(ground_truth, predicted, k):\n",
    "    \"\"\"Compute Recall@k\"\"\"\n",
    "    if not ground_truth:\n",
    "        return 0.0\n",
    "    predicted = predicted[:k]\n",
    "    relevant = set(ground_truth)\n",
    "    hits = sum(1 for item in predicted if item in relevant)\n",
    "    return hits / len(ground_truth)\n",
    "\n",
    "\n",
    "def ap_at_k(ground_truth, predicted, k):\n",
    "    \"\"\"Compute Average Precision@k\"\"\"\n",
    "    if not ground_truth or k == 0:\n",
    "        return 0.0\n",
    "\n",
    "    predicted = predicted[:k]\n",
    "    relevant = set(ground_truth)\n",
    "    hits = 0\n",
    "    sum_precisions = 0.0\n",
    "\n",
    "    for i, item in enumerate(predicted, 1):\n",
    "        if item in relevant:\n",
    "            hits += 1\n",
    "            sum_precisions += hits / i\n",
    "\n",
    "    return sum_precisions / min(len(ground_truth), k)\n",
    "\n",
    "\n",
    "def map_at_k(ground_truth_list, predicted_list, k):\n",
    "    \"\"\"Compute Mean Average Precision@k across multiple queries\"\"\"\n",
    "    return np.mean(\n",
    "        [ap_at_k(gt, pred, k) for gt, pred in zip(ground_truth_list, predicted_list)]\n",
    "    )\n",
    "\n",
    "\n",
    "def recall_rate_at_k(ground_truth_list, predicted_list, k):\n",
    "    \"\"\"Compute Mean Recall Rate@k across multiple queries\"\"\"\n",
    "    return np.mean(\n",
    "        [\n",
    "            recall_at_k(gt, pred[:k], k)\n",
    "            for gt, pred in zip(ground_truth_list, predicted_list)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "def dcg_at_k(ground_truth, predicted, k):\n",
    "    \"\"\"Compute Discounted Cumulative Gain@k\"\"\"\n",
    "    predicted = predicted[:k]\n",
    "    relevant = set(ground_truth)\n",
    "    gains = [\n",
    "        1.0 / np.log2(i + 2) if item in relevant else 0.0\n",
    "        for i, item in enumerate(predicted)\n",
    "    ]\n",
    "    return sum(gains)\n",
    "\n",
    "\n",
    "def ndcg_at_k(ground_truth, predicted, k):\n",
    "    \"\"\"Compute Normalized DCG@k\"\"\"\n",
    "    idcg = dcg_at_k(ground_truth, ground_truth, k)\n",
    "    if idcg == 0:\n",
    "        return 0.0\n",
    "    dcg = dcg_at_k(ground_truth, predicted, k)\n",
    "    return dcg / idcg\n",
    "\n",
    "\n",
    "def mean_ndcg_at_k(ground_truth_list, predicted_list, k):\n",
    "    \"\"\"Compute Mean nDCG@k across multiple queries\"\"\"\n",
    "    return np.mean(\n",
    "        [ndcg_at_k(gt, pred, k) for gt, pred in zip(ground_truth_list, predicted_list)]\n",
    "    )\n",
    "\n",
    "\n",
    "def mrr_at_k(ground_truth_list, predicted_list, k):\n",
    "    \"\"\"Compute Mean Reciprocal Rank@k across multiple queries\"\"\"\n",
    "    rr_scores = []\n",
    "    for gt, pred in zip(ground_truth_list, predicted_list):\n",
    "        pred = pred[:k]\n",
    "        relevant = set(gt)\n",
    "        for i, item in enumerate(pred, 1):\n",
    "            if item in relevant:\n",
    "                rr_scores.append(1.0 / i)\n",
    "                break\n",
    "        else:\n",
    "            rr_scores.append(0.0)\n",
    "    return np.mean(rr_scores)\n",
    "\n",
    "\n",
    "def f1_at_k(ground_truth_list, predicted_list, k):\n",
    "    \"\"\"Compute F1-score@k across multiple queries\"\"\"\n",
    "    f1_scores = []\n",
    "    for gt, pred in zip(ground_truth_list, predicted_list):\n",
    "        p = precision_at_k(gt, pred, k)\n",
    "        r = recall_at_k(gt, pred, k)\n",
    "        if (p + r) == 0:\n",
    "            f1_scores.append(0.0)\n",
    "        else:\n",
    "            f1_scores.append(2 * (p * r) / (p + r))\n",
    "    return np.mean(f1_scores)\n",
    "\n",
    "\n",
    "def compute_query_general_metrics(\n",
    "    ground_truth_list: list[list[int]], results_list: list[list[int]], k_values: list[int]\n",
    ") -> dict:\n",
    "    results = {\n",
    "        str(k): {\n",
    "            \"MAP\": map_at_k(ground_truth_list, results_list, k),\n",
    "            \"MAR\": recall_rate_at_k(ground_truth_list, results_list, k),\n",
    "            \"nDCG\": mean_ndcg_at_k(ground_truth_list, results_list, k),\n",
    "            \"MRR\": mrr_at_k(ground_truth_list, results_list, k),\n",
    "            \"F1\": f1_at_k(ground_truth_list, results_list, k),\n",
    "        }\n",
    "        for k in k_values\n",
    "    }\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "evil + inverted_idx: 100%|██████████| 24/24 [00:00<00:00, 449389.71it/s]\n",
      "evil + llm_tree_idx: 100%|██████████| 24/24 [00:00<00:00, 384211.05it/s]\n",
      "qwen-2-72b + inverted_idx: 100%|██████████| 24/24 [00:00<00:00, 337796.30it/s]\n",
      "qwen-2-72b + llm_tree_idx:   0%|          | 0/24 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "qwen-2-72b + llm_tree_idx: 100%|██████████| 24/24 [00:00<00:00, 379861.49it/s]\n",
      "inverted_idx: 100%|██████████| 24/24 [00:00<00:00, 384211.05it/s]\n",
      "llm_tree_idx: 100%|██████████| 24/24 [00:00<00:00, 432031.31it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'evil + inverted_idx': {'1': {'MAP': 0.2916666666666667,\n",
       "   'MAR': 0.11284722222222221,\n",
       "   'nDCG': 0.2916666666666667,\n",
       "   'MRR': 0.2916666666666667,\n",
       "   'F1': 0.14953703703703702},\n",
       "  '3': {'MAP': 0.16898148148148148,\n",
       "   'MAR': 0.14756944444444445,\n",
       "   'nDCG': 0.19943100032593122,\n",
       "   'MRR': 0.2916666666666667,\n",
       "   'F1': 0.1323773448773449},\n",
       "  '5': {'MAP': 0.15387731481481481,\n",
       "   'MAR': 0.1579861111111111,\n",
       "   'nDCG': 0.1870040812834205,\n",
       "   'MRR': 0.2916666666666667,\n",
       "   'F1': 0.1085927960927961},\n",
       "  '10': {'MAP': 0.1507523148148148,\n",
       "   'MAR': 0.1579861111111111,\n",
       "   'nDCG': 0.1834116853456722,\n",
       "   'MRR': 0.2916666666666667,\n",
       "   'F1': 0.06768463018463018}},\n",
       " 'evil + llm_tree_idx': {'1': {'MAP': 0.8333333333333334,\n",
       "   'MAR': 0.4288194444444444,\n",
       "   'nDCG': 0.8333333333333334,\n",
       "   'MRR': 0.8333333333333334,\n",
       "   'F1': 0.5342592592592593},\n",
       "  '3': {'MAP': 0.5648148148148148,\n",
       "   'MAR': 0.5517361111111111,\n",
       "   'nDCG': 0.649941831072539,\n",
       "   'MRR': 0.8888888888888888,\n",
       "   'F1': 0.4362463924963926},\n",
       "  '5': {'MAP': 0.5512152777777778,\n",
       "   'MAR': 0.5864583333333334,\n",
       "   'nDCG': 0.6397284228633935,\n",
       "   'MRR': 0.8888888888888888,\n",
       "   'F1': 0.3462581400081401},\n",
       "  '10': {'MAP': 0.5480902777777777,\n",
       "   'MAR': 0.5864583333333334,\n",
       "   'nDCG': 0.6361360269256452,\n",
       "   'MRR': 0.8888888888888888,\n",
       "   'F1': 0.20852850852850854}},\n",
       " 'qwen-2-72b + inverted_idx': {'1': {'MAP': 0.2916666666666667,\n",
       "   'MAR': 0.11284722222222221,\n",
       "   'nDCG': 0.2916666666666667,\n",
       "   'MRR': 0.2916666666666667,\n",
       "   'F1': 0.14953703703703702},\n",
       "  '3': {'MAP': 0.18055555555555555,\n",
       "   'MAR': 0.15451388888888887,\n",
       "   'nDCG': 0.20577322716856303,\n",
       "   'MRR': 0.2916666666666667,\n",
       "   'F1': 0.13515512265512267},\n",
       "  '5': {'MAP': 0.15763888888888888,\n",
       "   'MAR': 0.15451388888888887,\n",
       "   'nDCG': 0.18634099411475571,\n",
       "   'MRR': 0.2916666666666667,\n",
       "   'F1': 0.10082163207163208},\n",
       "  '10': {'MAP': 0.1597222222222222,\n",
       "   'MAR': 0.16493055555555555,\n",
       "   'nDCG': 0.1885426056566231,\n",
       "   'MRR': 0.2916666666666667,\n",
       "   'F1': 0.06821881821881823}},\n",
       " 'qwen-2-72b + llm_tree_idx': {'1': {'MAP': 0.7916666666666666,\n",
       "   'MAR': 0.4236111111111111,\n",
       "   'nDCG': 0.7916666666666666,\n",
       "   'MRR': 0.7916666666666666,\n",
       "   'F1': 0.525},\n",
       "  '3': {'MAP': 0.5555555555555556,\n",
       "   'MAR': 0.5291666666666667,\n",
       "   'nDCG': 0.6261692765846933,\n",
       "   'MRR': 0.8333333333333334,\n",
       "   'F1': 0.4211309523809524},\n",
       "  '5': {'MAP': 0.5458333333333334,\n",
       "   'MAR': 0.563888888888889,\n",
       "   'nDCG': 0.6200161565658527,\n",
       "   'MRR': 0.8333333333333334,\n",
       "   'F1': 0.3357142857142857},\n",
       "  '10': {'MAP': 0.5458333333333334,\n",
       "   'MAR': 0.563888888888889,\n",
       "   'nDCG': 0.6200161565658527,\n",
       "   'MRR': 0.8333333333333334,\n",
       "   'F1': 0.2023726273726274}},\n",
       " 'inverted_idx': {'1': {'MAP': 0.16666666666666666,\n",
       "   'MAR': 0.08680555555555557,\n",
       "   'nDCG': 0.16666666666666666,\n",
       "   'MRR': 0.16666666666666666,\n",
       "   'F1': 0.10694444444444444},\n",
       "  '3': {'MAP': 0.11342592592592593,\n",
       "   'MAR': 0.11284722222222221,\n",
       "   'nDCG': 0.13821105340831638,\n",
       "   'MRR': 0.19444444444444445,\n",
       "   'F1': 0.0946789321789322},\n",
       "  '5': {'MAP': 0.11180555555555555,\n",
       "   'MAR': 0.13368055555555555,\n",
       "   'nDCG': 0.13837817491346674,\n",
       "   'MRR': 0.19444444444444445,\n",
       "   'F1': 0.08230311355311358},\n",
       "  '10': {'MAP': 0.12219742063492063,\n",
       "   'MAR': 0.17881944444444445,\n",
       "   'nDCG': 0.15872692516282846,\n",
       "   'MRR': 0.20138888888888887,\n",
       "   'F1': 0.07462907462907463}},\n",
       " 'llm_tree_idx': {'1': {'MAP': 0.4583333333333333,\n",
       "   'MAR': 0.2638888888888889,\n",
       "   'nDCG': 0.4583333333333333,\n",
       "   'MRR': 0.4583333333333333,\n",
       "   'F1': 0.31805555555555554},\n",
       "  '3': {'MAP': 0.3888888888888889,\n",
       "   'MAR': 0.4427083333333333,\n",
       "   'nDCG': 0.46654025155069695,\n",
       "   'MRR': 0.611111111111111,\n",
       "   'F1': 0.3498376623376624},\n",
       "  '5': {'MAP': 0.4155555555555555,\n",
       "   'MAR': 0.534375,\n",
       "   'nDCG': 0.5020575225123012,\n",
       "   'MRR': 0.6194444444444445,\n",
       "   'F1': 0.32310999185999184},\n",
       "  '10': {'MAP': 0.4644642857142857,\n",
       "   'MAR': 0.7253472222222221,\n",
       "   'nDCG': 0.5799323943558328,\n",
       "   'MRR': 0.6375496031746032,\n",
       "   'F1': 0.26652606652606653}}}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_general_metrics(\n",
    "    general_metrics_path: Path = GENERAl_METRICS_PATH,\n",
    "    llm_res_path: Path = LLM_RES_PATH,\n",
    "    indexer_res_path: Path = INDEXER_RES_PATH,\n",
    "    k_values: list[int] = [1, 3, 5, 10],\n",
    "    force: bool = False,\n",
    ") -> dict:\n",
    "    llm_responses = load_json(llm_res_path)\n",
    "    indexer_responses = load_json(indexer_res_path)\n",
    "\n",
    "    if force:\n",
    "        remove_path(general_metrics_path)\n",
    "\n",
    "    general_metrics = load_json(general_metrics_path, allow_empty=True)\n",
    "\n",
    "    for model_name, queries in [*llm_responses.items(), *indexer_responses.items()]:\n",
    "        ground_truth_list, responses_list = [], []\n",
    "        for query_name, query_res in tqdm(queries.items(), desc=model_name):\n",
    "            if model_name not in general_metrics:\n",
    "                general_metrics[model_name] = {}\n",
    "\n",
    "            ground_truth_list.append(query_res[\"ground_truths_idx\"])\n",
    "            responses_list.append(query_res[\"responses_idx\"])\n",
    "        general_metrics[model_name] = compute_query_general_metrics(\n",
    "            ground_truth_list, responses_list, k_values\n",
    "        )\n",
    "\n",
    "    save_json(general_metrics_path, general_metrics)\n",
    "\n",
    "    return general_metrics\n",
    "\n",
    "\n",
    "general_metrics = compute_general_metrics(force=True)\n",
    "\n",
    "general_metrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
