{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6656b951",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c5575255",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "from sklearn.neighbors import BallTree\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "from utils import from_current_file, load\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0b97b0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTEmbedder:\n",
    "    def __init__(\n",
    "        self, model_name=\"sentence-transformers/msmarco-bert-base-dot-v5\", device=DEVICE\n",
    "    ):\n",
    "        self.device = device or \"cpu\"\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModel.from_pretrained(model_name).to(self.device)\n",
    "        self.model.eval()\n",
    "\n",
    "        self.do_lower_case = getattr(self.tokenizer, \"do_lower_case\", False)\n",
    "\n",
    "    def text_to_embedding(self, texts, pooling=\"mean\", normalize=False):\n",
    "        is_single = isinstance(texts, str)\n",
    "        texts = [texts] if is_single else texts\n",
    "\n",
    "        inputs = self.tokenizer(\n",
    "            texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=512\n",
    "        ).to(self.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "\n",
    "        if pooling == \"mean\":\n",
    "            mask = inputs[\"attention_mask\"].unsqueeze(-1)\n",
    "            embeddings = (outputs.last_hidden_state * mask).sum(1) / mask.sum(1).clamp(\n",
    "                min=1e-9\n",
    "            )\n",
    "        elif pooling == \"cls\":\n",
    "            embeddings = outputs.last_hidden_state[:, 0, :]\n",
    "        else:\n",
    "            raise ValueError(\"Invalid pooling method\")\n",
    "\n",
    "        if normalize:\n",
    "            embeddings = torch.nn.functional.normalize(embeddings, p=2, dim=1)\n",
    "\n",
    "        return embeddings.cpu().numpy()[0] if is_single else embeddings.cpu().numpy()\n",
    "\n",
    "\n",
    "def batch(iterable, batch_size):\n",
    "    all_batches = []\n",
    "    current_batch = []\n",
    "\n",
    "    for item in iterable:\n",
    "        current_batch.append(item)\n",
    "        if len(current_batch) == batch_size:\n",
    "            all_batches.append(current_batch)\n",
    "            current_batch = []\n",
    "\n",
    "    if current_batch:  # Add the last partial batch\n",
    "        all_batches.append(current_batch)\n",
    "\n",
    "    return all_batches\n",
    "\n",
    "\n",
    "class BERTBallTree:\n",
    "    def __init__(\n",
    "        self,\n",
    "        index_dir: str = \"../data/embedding_directory\",\n",
    "        documents_dir: str = \"../data/scrapped/class_data_function__1_1\",\n",
    "        embedder=None,\n",
    "        metric=\"euclidean\",\n",
    "        leaf_size=1,\n",
    "        tree_name=\"tree\",\n",
    "        force=False,\n",
    "    ):\n",
    "        self._index_dir = from_current_file(index_dir)\n",
    "        self._documents_dir = from_current_file(documents_dir)\n",
    "        self.embedder = embedder or BERTEmbedder()\n",
    "        self.metric = metric\n",
    "        self.leaf_size = leaf_size\n",
    "        self.tree = None\n",
    "        self.tree_name = tree_name\n",
    "        self.documents: dict[int, str] = {}\n",
    "\n",
    "        if force or not os.path.exists(self._index_dir):\n",
    "            print(\"Tree is not found, creating new...\")\n",
    "            if force:\n",
    "                try:\n",
    "                    shutil.rmtree(self._index_dir)\n",
    "                except FileNotFoundError:\n",
    "                    pass\n",
    "            os.mkdir(path=self._index_dir)\n",
    "            self.build_tree()\n",
    "            print(\"Complete!\")\n",
    "\n",
    "        self.load_tree()\n",
    "\n",
    "    def build_tree(self):\n",
    "        sentences = []\n",
    "        for document_id, filename in enumerate(os.listdir(self._documents_dir)):\n",
    "            if filename.endswith(\".txt\"):\n",
    "                with open(\n",
    "                    os.path.join(self._documents_dir, filename), \"r\", encoding=\"utf-8\"\n",
    "                ) as f:\n",
    "                    text = f.read()\n",
    "                    self.documents[document_id] = filename[:-4]\n",
    "                    sentences.append(text)\n",
    "\n",
    "        embeddings = []\n",
    "        for b in tqdm(batch(sentences, 64)):\n",
    "            batch_embeddings = self.embedder.text_to_embedding(\n",
    "                b, pooling=\"mean\", normalize=True\n",
    "            )\n",
    "            embeddings.extend(batch_embeddings)\n",
    "        self.tree = BallTree(embeddings, metric=self.metric, leaf_size=self.leaf_size)\n",
    "        self.save_tree()\n",
    "\n",
    "    def query(self, query_text, k=5, return_distances=False):\n",
    "        \"\"\"\n",
    "        Query the Ball Tree for nearest neighbors.\n",
    "\n",
    "        Args:\n",
    "            query_text: The query text string\n",
    "            k: Number of nearest neighbors to return\n",
    "            return_distances: Whether to return distances along with results\n",
    "\n",
    "        Returns:\n",
    "            If return_distances is False: list of nearest texts\n",
    "            If return_distances is True: tuple of (texts, distances)\n",
    "        \"\"\"\n",
    "        if self.tree is None:\n",
    "            raise ValueError(\"Ball Tree has not been built yet. Call build_tree() first.\")\n",
    "\n",
    "        # Get embedding for the query text\n",
    "        query_embedding = self.embedder.text_to_embedding(\n",
    "            query_text, pooling=\"mean\", normalize=True\n",
    "        ).reshape(1, -1)\n",
    "\n",
    "        # Query the tree\n",
    "        distances, indices = self.tree.query(query_embedding, k=k)\n",
    "\n",
    "        # Get the corresponding texts\n",
    "        results = []\n",
    "        for indice in indices[0]:\n",
    "            results.append(self.documents[indice])\n",
    "\n",
    "        if return_distances:\n",
    "            return results, distances[0]\n",
    "        return results\n",
    "\n",
    "    def save_tree(self):\n",
    "        \"\"\"Save the Ball Tree and associated data to disk.\"\"\"\n",
    "        import joblib\n",
    "\n",
    "        data = {\n",
    "            \"tree\": self.tree,\n",
    "            \"documents\": self.documents,\n",
    "        }\n",
    "        joblib.dump(data, os.path.join(self._index_dir, self.tree_name))\n",
    "\n",
    "    def load_tree(self):\n",
    "        \"\"\"Load a saved Ball Tree from disk.\"\"\"\n",
    "        import joblib\n",
    "\n",
    "        data = joblib.load(os.path.join(self._index_dir, self.tree_name))\n",
    "        self.tree = data[\"tree\"]\n",
    "        self.documents = data[\"documents\"]\n",
    "\n",
    "\n",
    "# Initialize and build the tree\n",
    "ball_tree = BERTBallTree()\n",
    "\n",
    "# Query the tree\n",
    "# query = \"sin\"\n",
    "# results, distances = ball_tree.query(query, k=10, return_distances=True)\n",
    "\n",
    "# print(f\"Query: {query}\")\n",
    "# print(\"Top 3 results:\")\n",
    "# for text, dist in zip(results, distances):\n",
    "# print(f\"- {text} (distance: {dist:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2263ba",
   "metadata": {},
   "source": [
    "### ADD INDEXER/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ed98b7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import torch\n",
    "from g4f.client import Client\n",
    "\n",
    "\n",
    "class RAG:\n",
    "    folder_path = \"../data/scrapped/class_data_function__1_1\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.client = Client()\n",
    "        # model_id = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "        # self.tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "        # self.model = AutoModelForCausalLM.from_pretrained(\n",
    "        #     model_id,\n",
    "        #     device_map=\"cuda\",\n",
    "        #     torch_dtype=\"auto\",\n",
    "        #     trust_remote_code=True\n",
    "        # )\n",
    "\n",
    "    def generate_stream(self, question: str, model: str, k: int = 10):\n",
    "        res, context = self._retrive_docs(question, k)\n",
    "        prompt = (\n",
    "            \"You're a Python expert. Suppose all the documentation information you know is provided in context section. \"\n",
    "            \"Answer on the question as usual but take technical information only from context. \"\n",
    "            'If there is no answer in the context, say, \"I can\\'t find the answer in the Python documentation\"\\n'\n",
    "            \"Highlight cited passages or provide “show sources” toggles ONLY FROM CONTEXT\\n\"\n",
    "            \"NO PYTHON CODE EXAMPLES!\\n\"\n",
    "            \"NO PROMPT REPETITION IN ANSWER!\\n\"\n",
    "            \"NO EXAMPLES!\\n\"\n",
    "            \"NO ADDITIONAL EXPLANATIONS!\\n\"\n",
    "            'If question is unrealated to python documentation, just answer \"Question is unrelated\"\\n'\n",
    "            \"\\nContext:\\n\"\n",
    "            f\"{'\\n\\n'.join([f'{idx + 1}. {c}' for idx, c in enumerate(context)])}\\n\"\n",
    "            f\"\\nQuestion: {question}\\n\"\n",
    "            f\"\\nResponse (with reference to the source [1-{k}]):\\n\"\n",
    "        )\n",
    "        messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "\n",
    "        yield (\n",
    "            json.dumps(\n",
    "                {\n",
    "                    \"type\": \"proposals\",\n",
    "                    \"data\": [{\"document\": x[0], \"score\": x[1]} for x in res],\n",
    "                }\n",
    "            )\n",
    "            + \"\\n\\n\"\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=model,\n",
    "                messages=messages,\n",
    "                stream=True,\n",
    "                verbose=False,\n",
    "                max_tokens=200,\n",
    "            )\n",
    "\n",
    "            for chunk in response:\n",
    "                if chunk.choices[0].delta.content:\n",
    "                    yield (\n",
    "                        json.dumps(\n",
    "                            {\"type\": \"chunk\", \"data\": chunk.choices[0].delta.content}\n",
    "                        )\n",
    "                        + \"\\n\\n\"\n",
    "                    )\n",
    "        except BaseException as e:\n",
    "            yield json.dumps({\"type\": \"error\", \"data\": str(e)}) + \"\\n\\n\"\n",
    "\n",
    "    def _retrive_docs(self, query_m: str, k):\n",
    "        results, distances = ball_tree.query(query_m, k, return_distances=True)\n",
    "        print(results, distances)\n",
    "\n",
    "        contents = []\n",
    "\n",
    "        for name in results:\n",
    "            content = load(os.path.join(self.folder_path, name + \".txt\"))\n",
    "\n",
    "            contents.append(name + \"\\n\" + content)\n",
    "\n",
    "        return [results, distances], contents\n",
    "\n",
    "    def get_answer(self, question: str, model: str, local: bool = True, k: int = 10):\n",
    "        _, context = self._retrive_docs(question, k)\n",
    "\n",
    "        prompt = (\n",
    "            \"You're a Python expert. Suppose all the documentation information you know is provided in context section. \"\n",
    "            \"Answer on the question as usual but take technical information only from context. \"\n",
    "            'If there is no answer in the context, say, \"I can\\'t find the answer in the Python documentation\"\\n'\n",
    "            \"Highlight cited passages or provide “show sources” toggles ONLY FROM CONTEXT\\n\"\n",
    "            \"NO PYTHON CODE EXAMPLES!\\n\"\n",
    "            \"NO PROMPT REPETITION IN ANSWER!\\n\"\n",
    "            \"NO EXAMPLES!\\n\"\n",
    "            \"NO ADDITIONAL EXPLANATIONS!\\n\"\n",
    "            'If question is unrealated to python documentation, just answer \"Question is unrelated\"\\n'\n",
    "            \"\\nContext:\\n\"\n",
    "            f\"{'\\n\\n'.join([f'{idx + 1}. {c}' for idx, c in enumerate(context)])}\\n\"\n",
    "            f\"\\nQuestion: {question}\\n\"\n",
    "            f\"\\nResponse (with reference to the source [1-{k}]):\\n\"\n",
    "        )\n",
    "        # print(prompt)\n",
    "        messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "\n",
    "        try:\n",
    "            if not local:\n",
    "                response = self.client.chat.completions.create(\n",
    "                    model=model, messages=messages, web_search=False, stream=False\n",
    "                )\n",
    "                return response.choices[0].message.content, \"\"\n",
    "            else:\n",
    "                inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "                outputs = self.model.generate(\n",
    "                    **inputs, max_new_tokens=200, use_cache=False\n",
    "                )\n",
    "                return self.tokenizer.decode(outputs[0], skip_special_tokens=True), \"\"\n",
    "        except BaseException as e:\n",
    "            return \"\", str(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3367be08",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_model = RAG()\n",
    "question = \"How to find goth\"\n",
    "\n",
    "# res, err = rag_model.get_answer(\n",
    "#     question=question, model=\"evil\", client=client, local=False, k=3\n",
    "# )\n",
    "# print(res)\n",
    "# print(err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "855deee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['wave.Wave_write.setcomptype', 'wave.Wave_write.setsampwidth', 'wave.Wave_write.setframerate', 'wave.Wave_read', 'wave.Wave_read.setpos'] [0.47908269 0.49011799 0.49325316 0.49424956 0.49997578]\n",
      "{\"type\": \"proposals\", \"data\": [{\"document\": \"wave.Wave_write.setcomptype\", \"score\": \"wave.Wave_write.setsampwidth\"}, {\"document\": 0.47908268763074147, \"score\": 0.4901179886101874}]}\n",
      "\n",
      "\n",
      "{\"type\": \"error\", \"data\": \"\"}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for res in rag_model.generate_stream(\"Write bublesort algorithm\", model=\"command-r\", k=5):\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "56ac46ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['turtle.title', 'turtle.setup', 'turtle.shapetransform', 'turtle.resetscreen', 'turtle.forward'] [0.38961707 0.40827205 0.41424621 0.41622499 0.41691331]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(\"I can't find the answer in the Python documentation. However, the context provided includes several functions related to the turtle module: `turtle.title` [1], `turtle.setup` [2], `turtle.shapetransform` [3], `turtle.resetscreen` [4], and `turtle.forward` [5]. These functions allow you to manipulate the turtle graphics window, such as setting its title, size, and position, transforming the turtle shape, resetting the screen, and moving the turtle forward.\",\n",
       " '')"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_model.get_answer(\"turtle\", model=\"qwen-2-72b\", k=5, local=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd6d380b",
   "metadata": {},
   "source": [
    "- command-r\n",
    "- evil\n",
    "- qwen-2-72b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fe086b1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You\\'re a Python expert. Suppose all the documentation information you know is provided in context section. Answer on the question as usual but take technical information only from context. If there is no answer in the context, say, \"I can\\'t find the answer in the Python documentation\"\\nHighlight cited passages ONLY FROM CONTEXT\\nNO PYTHON CODE EXAMPLES!\\nNO PROMPT REPETITION IN ANSWER!\\n\\nContext:\\n1. math.pow\\nFUNCTION\\n\\nmath.pow FROM math\\n\\nPARAMETERS\\nx, y\\n\\nDESCRIPTION\\nReturn x raised to the power y.  Exceptional cases follow\\nthe IEEE 754 standard as far as possible.  In particular,\\npow(1.0, x) and pow(x, 0.0) always return 1.0, even\\nwhen x is a zero or a NaN.  If both x and y are finite,\\nx is negative, and y is not an integer then pow(x, y)\\nis undefined, and raises ValueError.\\nUnlike the built-in ** operator, math.pow() converts both\\nits arguments to type float.  Use ** or the built-in\\npow() function for computing exact integer powers.\\nChanged in version 3.11: The special cases pow(0.0, -inf) and pow(-0.0, -inf) were\\nchanged to return inf instead of raising ValueError,\\nfor consistency with IEEE 754.\\n\\n2. cmath.sin\\nFUNCTION\\n\\ncmath.sin FROM cmath\\n\\nPARAMETERS\\nz\\n\\nDESCRIPTION\\nReturn the sine of z.\\n\\n3. math.prod\\nFUNCTION\\n\\nmath.prod FROM math\\n\\nPARAMETERS\\niterable, *, start=1\\n\\nDESCRIPTION\\nCalculate the product of all the elements in the input iterable.\\nThe default start value for the product is 1.\\nWhen the iterable is empty, return the start value.  This function is\\nintended specifically for use with numeric values and may reject\\nnon-numeric types.\\nAdded in version 3.8.\\n\\nQuestion: Sin\\n\\nResponse (with reference to the source [1-3]):\\n\\nThe sine of a number can be calculated using the cmath.sin function from the cmath module.\\n\\nFor example:\\n\\n```python\\nimport cmath\\n\\n# Calculate the sine of pi/2\\nresult = cmath.sin(cmath.pi / 2)\\nprint(result)\\n```\\n\\nThis will output:\\n\\n```\\n1.0\\n```\\n\\nThis is because the sine of pi/2 radians (or 90 degrees) is 1.\\n\\nNote: The cmath module is used for complex numbers, so if you\\'re working with real numbers, you can use the math.sin function from the math module instead.\\n\\nFor example:\\n\\n```python\\nimport math\\n\\n# Calculate the sine of pi/2\\nresult = math.sin(math.pi / 2)\\nprint(result)\\n```\\n\\nThis will also output'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3d950a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "# import torch\n",
    "\n",
    "# model_id = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     model_id,\n",
    "#     device_map=\"cuda\",\n",
    "#     torch_dtype=\"auto\",\n",
    "#     trust_remote_code=True\n",
    "# )\n",
    "\n",
    "# # Disable cache (if not needed)\n",
    "# inputs = tokenizer(\"Explain RAG.\", return_tensors=\"pt\").to(\"cuda\")\n",
    "# outputs = model.generate(**inputs, max_new_tokens=200, use_cache=False)  # ← Critical!\n",
    "# print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3e6b49",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
