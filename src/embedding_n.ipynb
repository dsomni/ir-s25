{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\nikit\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\nikit\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import nltk\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.neighbors import KDTree\n",
    "from utils import from_current_file, load_json, round_float, save_json\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"punkt_tab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index is not found, creating new...\n",
      "Complete!\n"
     ]
    }
   ],
   "source": [
    "class Word2VecIndexer:\n",
    "    _stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        index_dir: str = \"../data/embedding_directory\",\n",
    "        documents_dir: str = \"../data/scrapped/class_data_function__1_1\",\n",
    "        top_similar: int = 10,\n",
    "        force: bool = False,\n",
    "    ):\n",
    "        \n",
    "        self._index_dir = from_current_file(index_dir)\n",
    "        self._documents_dir = from_current_file(documents_dir)\n",
    "        self.top_similar = top_similar\n",
    "\n",
    "        self._word2vec_model_path = os.path.join(self._index_dir, \"word2vec.model\")\n",
    "        self._annoy_index_path = os.path.join(self._index_dir, \"doc_embeddings.ann\")\n",
    "        self.doc_embeddings: dict[int, np.ndarray] = {}  # Document ID -> embedding\n",
    "        # self.annoy_index: AnnoyIndex = None  # Annoy index for document embeddings\n",
    "        self._doc_id_path = os.path.join(self._index_dir, \"documents.json\")\n",
    "        self.documents: dict[int, str] = {}\n",
    "\n",
    "        self.model: Word2Vec = None\n",
    "        self.kd_tree = None\n",
    "\n",
    "\n",
    "        if force or not os.path.exists(self._index_dir):\n",
    "            print(\"Index is not found, creating new...\")\n",
    "            if force:\n",
    "                try:\n",
    "                    shutil.rmtree(self._index_dir)\n",
    "                except FileNotFoundError:\n",
    "                    pass\n",
    "            os.mkdir(path=self._index_dir)\n",
    "            self.build_index()\n",
    "            print(\"Complete!\")\n",
    "\n",
    "        self.load_index()\n",
    "\n",
    "    def _tokenize(self, text: str) -> list[str]:\n",
    "        return [w for w in re.findall(r\"\\w+\", text.lower()) if w not in self._stop_words]\n",
    "\n",
    "    def _get_similar_words(self, word: str) -> set[tuple[str, float]]:\n",
    "        matches = set()\n",
    "        if self.model and word in self.model.wv:\n",
    "            for similar_word, similarity in self.model.wv.most_similar(\n",
    "                word, topn=self.top_similar, indexer=self.annoy_indexer\n",
    "            ):\n",
    "                if similar_word in self.index:\n",
    "                    matches.add((similar_word, similarity))\n",
    "        return matches\n",
    "\n",
    "    def build_index(self):\n",
    "        sentences = []\n",
    "        for document_id, filename in enumerate(os.listdir(self._documents_dir)):\n",
    "            if filename.endswith(\".txt\"):\n",
    "                with open(\n",
    "                    os.path.join(self._documents_dir, filename), \"r\", encoding=\"utf-8\"\n",
    "                ) as f:\n",
    "                    text = f.read()\n",
    "                    self.documents[document_id] = filename[:-4]\n",
    "                    words = self._tokenize(text)\n",
    "                    sentences.append(words)\n",
    "\n",
    "        self.model = Word2Vec(\n",
    "            sentences=sentences,\n",
    "            vector_size=1000,\n",
    "            min_count = 1,\n",
    "            window=10,\n",
    "            workers=4,\n",
    "        )\n",
    "\n",
    "        # vector_size = self.model.vector_size\n",
    "        self.doc_embeddings = {\n",
    "            doc_id: np.mean([\n",
    "                self.model.wv[word] \n",
    "                for word in words \n",
    "                if word in self.model.wv\n",
    "            ], axis=0) \n",
    "            for doc_id, words in enumerate(sentences)\n",
    "        }\n",
    "\n",
    "        self.kd_tree = KDTree(list(self.doc_embeddings.values()))\n",
    "        \n",
    "        # # Build Annoy index for documents\n",
    "        # self.annoy_index = AnnoyIndex(vector_size, 'angular')\n",
    "  \n",
    "\n",
    "        # Persist model and index\n",
    "        self.model.save(self._word2vec_model_path)\n",
    "\n",
    "        save_json(self._doc_id_path, self.documents)\n",
    "\n",
    "    def load_index(self):\n",
    "        self.documents = {int(k): v for k, v in load_json(self._doc_id_path).items()}\n",
    "        self.model = Word2Vec.load(self._word2vec_model_path)\n",
    "\n",
    "    def find(self, query: str, top_k: int = 10) -> list:\n",
    "        query_words = self._tokenize(query)\n",
    "        query_vectors = [\n",
    "            self.model.wv[word] \n",
    "            for word in query_words \n",
    "            if word in self.model.wv\n",
    "        ]\n",
    "        \n",
    "        if not query_vectors:\n",
    "            return []\n",
    "        \n",
    "        distance, doc_id = self.kd_tree.query(query_vectors, k=top_k)\n",
    "        # Average word vectors for query embedding\n",
    "        # query_embedding = np.mean(query_vectors, axis=0)\n",
    "        \n",
    "        # # Find similar documents using Annoy\n",
    "        # doc_ids, distances = self.annoy_index.get_nns_by_vector(\n",
    "        #     query_embedding, \n",
    "        #     top_k, \n",
    "        #     include_distances=True\n",
    "        # )\n",
    "        \n",
    "        # # Convert angular distances to cosine similarities\n",
    "\n",
    "        return [(self.documents[idx], dist) \n",
    "            for idx, dist in zip(doc_id[0], distance[0])]\n",
    "\n",
    "\n",
    "\n",
    "indexer = Word2VecIndexer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 2.216838800105832\tFile: decimal.MIN_EMIN\n",
      "Score: 2.220138559268754\tFile: decimal.MAX_EMAX\n",
      "Score: 2.2211038292057674\tFile: decimal.MIN_ETINY\n",
      "Score: 2.22313474160379\tFile: decimal.MAX_PREC\n",
      "Score: 2.296588067886844\tFile: math.perm\n",
      "Score: 2.3070206271299036\tFile: math.comb\n",
      "Score: 2.576514269978188\tFile: math.cbrt\n",
      "Score: 2.5767479067604695\tFile: math.sin\n",
      "Score: 2.5789153502662243\tFile: math.pow\n",
      "Score: 2.5888474751138157\tFile: math.sinh\n"
     ]
    }
   ],
   "source": [
    "results = indexer.find('''math sin ''')\n",
    "for doc, score in results:\n",
    "    print(f\"Score: {score}\\tFile: {doc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
