{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import logging\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "\n",
    "import nest_asyncio\n",
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.translate.bleu_score import SmoothingFunction, sentence_bleu\n",
    "from rouge import Rouge\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm import tqdm\n",
    "\n",
    "ROOT_FOLDERS = [os.path.join(\".\", \"..\"), os.path.join(\".\", \"..\", \"..\")]\n",
    "for folder in ROOT_FOLDERS:\n",
    "    if folder not in sys.path:\n",
    "        sys.path.insert(0, folder)\n",
    "\n",
    "from src.pipeline import ApiModel, Indexer, IndexerPipeline, RAGPipeline\n",
    "from src.utils import from_current_file, load, load_json, remove_path, save_json\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "logging.disable(logging.WARNING)\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = from_current_file(\"../data/scrapped/class_data_function__1_1\")\n",
    "\n",
    "NAME_TO_IDX = {\n",
    "    filename[:-4]: document_id\n",
    "    for document_id, filename in enumerate(os.listdir(DATA_PATH))\n",
    "}\n",
    "IDX_TO_NAME = {v: k for k, v in NAME_TO_IDX.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "INDEXER_PIPELINE = IndexerPipeline()\n",
    "RAG_PIPELINE = RAGPipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LLM_RES_PATH = from_current_file(\"../data/evaluation/llm_responses.json\")\n",
    "QUERIES_PATH = from_current_file(\"../data/evaluation/queries.json\")\n",
    "INDEXER_RES_PATH = from_current_file(\"../data/evaluation/indexer_responses.json\")\n",
    "\n",
    "LLM_METRICS_PATH = from_current_file(\"../data/evaluation/llm_metrics.json\")\n",
    "GENERAl_METRICS_PATH = from_current_file(\"../data/evaluation/general_metrics.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_queries(\n",
    "    path: Path = QUERIES_PATH,\n",
    ") -> list[tuple[str, str, list[str]]]:\n",
    "    return [(k, v[\"query\"], v[\"ground_truths\"]) for k, v in load_json(path).items()]\n",
    "\n",
    "\n",
    "queries = load_queries()\n",
    "# queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:01<00:00,  1.61it/s]\n"
     ]
    }
   ],
   "source": [
    "def generate_indexer_responses(\n",
    "    indexer_res_path: Path = INDEXER_RES_PATH,\n",
    "    force: bool = False,\n",
    ") -> dict:\n",
    "    queries = load_queries()\n",
    "\n",
    "    indexers = INDEXER_PIPELINE.available_indexers\n",
    "    if force:\n",
    "        remove_path(indexer_res_path)\n",
    "    indexer_responses = load_json(indexer_res_path, allow_empty=True)\n",
    "\n",
    "    for query_name, query, ground_truths in tqdm(queries):\n",
    "        for indexer in indexers:\n",
    "            if indexer not in indexer_responses:\n",
    "                indexer_responses[indexer] = {}\n",
    "            if query_name in indexer_responses[indexer]:\n",
    "                continue\n",
    "            _, docs_scores = INDEXER_PIPELINE.index(query, indexer, k=10)\n",
    "            docs = [doc for doc, _ in docs_scores]\n",
    "            indexer_responses[indexer][query_name] = {\n",
    "                \"query\": query_name,\n",
    "                \"responses\": docs,\n",
    "                \"responses_idx\": [NAME_TO_IDX[d] for d in docs],\n",
    "                \"ground_truths\": ground_truths,\n",
    "                \"ground_truths_idx\": [NAME_TO_IDX[d] for d in ground_truths],\n",
    "            }\n",
    "    save_json(indexer_res_path, indexer_responses)\n",
    "    return indexer_responses\n",
    "\n",
    "\n",
    "indexer_responses = generate_indexer_responses(force=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_from_response(text: str) -> list[int]:\n",
    "    try:\n",
    "        # Find all matches\n",
    "        ref_matches = re.findall(r\"\\[(\\d+(?:,\\s*\\d+)*)\\]\", text.split(\"References:\")[1])\n",
    "        # ref_matches = re.findall(pattern, text)\n",
    "\n",
    "        # Extract individual numbers\n",
    "        numbers = []\n",
    "        for match in ref_matches:\n",
    "            numbers.extend([num.strip() for num in match.split(\",\")])\n",
    "\n",
    "        # Convert to integers\n",
    "        numbers = list(map(int, numbers))\n",
    "        return numbers\n",
    "    except BaseException:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def access_llm(\n",
    "    query: str, model: ApiModel, indexer: Indexer, model_name: str, k=10\n",
    ") -> tuple[str, list[str], str, Optional[str]]:\n",
    "    try:\n",
    "        response, sources = await RAG_PIPELINE.request_async(\n",
    "            query, model, k=k, indexer=indexer\n",
    "        )\n",
    "        return response, sources, model_name, None\n",
    "    except Exception as e:\n",
    "        return \"\", [], model_name, str(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:19<00:00,  9.75s/it]\n"
     ]
    }
   ],
   "source": [
    "async def generate_llm_responses(\n",
    "    models: list[ApiModel],\n",
    "    timeout: float = 60.0,\n",
    "    llm_res_path: Path = LLM_RES_PATH,\n",
    "    force: bool = False,\n",
    ") -> dict:\n",
    "    queries = load_queries()\n",
    "\n",
    "    indexers = INDEXER_PIPELINE.available_indexers\n",
    "    if force:\n",
    "        remove_path(llm_res_path)\n",
    "\n",
    "    llm_responses = load_json(llm_res_path, allow_empty=True)\n",
    "\n",
    "    for query_name, query, ground_truths in tqdm(queries):\n",
    "        results = []\n",
    "        for model in models:\n",
    "            for indexer in indexers:\n",
    "                model_name = f\"{model} + {indexer}\"\n",
    "                if model_name not in llm_responses:\n",
    "                    llm_responses[model_name] = {}\n",
    "                if query_name in llm_responses[model_name]:\n",
    "                    continue\n",
    "                # print(f\"Processing {model_name}...\")\n",
    "\n",
    "                results.append(access_llm(query, model, indexer, model_name))\n",
    "\n",
    "        for coro in asyncio.as_completed(results):\n",
    "            try:\n",
    "                llm_res, sources, model_name, err = await asyncio.wait_for(coro, timeout)\n",
    "                if err is not None:\n",
    "                    continue\n",
    "                local_indices = extract_from_response(llm_res)\n",
    "                docs = [sources[idx - 1] for idx in local_indices]\n",
    "                llm_responses[model_name][query_name] = {\n",
    "                    \"query\": query_name,\n",
    "                    \"llm_output\": llm_res,\n",
    "                    \"sources\": sources,\n",
    "                    \"sources_idx\": [NAME_TO_IDX[d] for d in sources],\n",
    "                    \"responses\": docs,\n",
    "                    \"responses_idx\": [NAME_TO_IDX[d] for d in docs],\n",
    "                    \"ground_truths\": ground_truths,\n",
    "                    \"ground_truths_idx\": [NAME_TO_IDX[d] for d in ground_truths],\n",
    "                }\n",
    "            except asyncio.TimeoutError:\n",
    "                continue\n",
    "            except BaseException:\n",
    "                continue\n",
    "        await asyncio.sleep(2)\n",
    "\n",
    "    save_json(llm_res_path, llm_responses)\n",
    "    return llm_responses\n",
    "\n",
    "\n",
    "llm_responses = await generate_llm_responses([\"evil\", \"qwen-2-72b\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_sim(a, b):\n",
    "    return cosine_similarity([a], [b])[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_context(doc_names: list[str]) -> list[str]:\n",
    "    return [load(os.path.join(DATA_PATH, f\"{doc_name}.txt\")) for doc_name in doc_names]\n",
    "\n",
    "\n",
    "def compute_query_llm_metrics(query_result: dict, get_embedding):\n",
    "    rouge = Rouge()\n",
    "    smooth = SmoothingFunction().method1\n",
    "\n",
    "    query = query_result[\"query\"]\n",
    "    llm_output = query_result[\"llm_output\"]\n",
    "    contexts = build_context(query_result[\"responses\"])\n",
    "    ground_truths = \"\\n\".join(query_result[\"ground_truths\"])\n",
    "\n",
    "    emb_query = get_embedding(query)\n",
    "    emb_llm_output = get_embedding(llm_output)\n",
    "    emb_ground_truths = get_embedding(ground_truths)\n",
    "    emb_contexts = [get_embedding(c) for c in contexts]\n",
    "    emb_context_all = get_embedding(\" \".join(contexts))\n",
    "\n",
    "    # 1. faithfulness: answer vs context\n",
    "    faithfulness_score = cosine_sim(emb_llm_output, emb_context_all)\n",
    "\n",
    "    # 2. answer relevancy: answer vs question\n",
    "    answer_relevancy_score = cosine_sim(emb_llm_output, emb_query)\n",
    "\n",
    "    # 3. context recall: ground truth vs context\n",
    "    context_recall_score = cosine_sim(emb_ground_truths, emb_context_all)\n",
    "\n",
    "    # 4. context precision: answer vs each context (max similarity)\n",
    "    precisions = [cosine_sim(emb_llm_output, ctx_emb) for ctx_emb in emb_contexts]\n",
    "    context_precision_score = np.max(precisions) if precisions else 0.0\n",
    "\n",
    "    # 5. BLEU: answer vs ground truth\n",
    "    bleu_score = sentence_bleu(\n",
    "        [nltk.word_tokenize(ground_truths.lower())],\n",
    "        nltk.word_tokenize(llm_output.lower()),\n",
    "        smoothing_function=smooth,\n",
    "        weights=(0.5, 0.5),  # BLEU-2\n",
    "    )\n",
    "\n",
    "    # 6. ROUGE: answer vs ground truth\n",
    "    rouge_scores = rouge.get_scores(llm_output, ground_truths)[0]\n",
    "    rouge1 = rouge_scores[\"rouge-1\"][\"f\"]\n",
    "    rougeL = rouge_scores[\"rouge-l\"][\"f\"]\n",
    "\n",
    "    return {\n",
    "        \"faithfulness\": round(faithfulness_score, 3),\n",
    "        \"answer_relevancy\": round(answer_relevancy_score, 3),\n",
    "        \"context_recall\": round(context_recall_score, 3),\n",
    "        \"context_precision\": round(context_precision_score, 3),\n",
    "        \"bleu\": round(bleu_score, 3),\n",
    "        \"rouge1\": round(rouge1, 3),\n",
    "        \"rougeL\": round(rougeL, 3),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "evil + inverted_idx: 100%|██████████| 2/2 [00:00<00:00,  3.54it/s]\n",
      "evil + llm_tree_idx: 100%|██████████| 2/2 [00:02<00:00,  1.29s/it]\n",
      "qwen-2-72b + inverted_idx: 100%|██████████| 2/2 [00:05<00:00,  2.56s/it]\n",
      "qwen-2-72b + llm_tree_idx: 100%|██████████| 2/2 [00:02<00:00,  1.48s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'evil + inverted_idx': {'0': {'faithfulness': 0.75,\n",
       "   'answer_relevancy': 0.096,\n",
       "   'context_recall': 0.724,\n",
       "   'context_precision': 0.661,\n",
       "   'bleu': 0.007,\n",
       "   'rouge1': 0.0,\n",
       "   'rougeL': 0.0},\n",
       "  '1': {'faithfulness': 0.69,\n",
       "   'answer_relevancy': 0.144,\n",
       "   'context_recall': 0.446,\n",
       "   'context_precision': 0.69,\n",
       "   'bleu': 0,\n",
       "   'rouge1': 0.0,\n",
       "   'rougeL': 0.0},\n",
       "  'mean': {'faithfulness': 0.72,\n",
       "   'answer_relevancy': 0.12,\n",
       "   'context_recall': 0.585,\n",
       "   'context_precision': 0.67550004,\n",
       "   'bleu': 0.0035,\n",
       "   'rouge1': 0.0,\n",
       "   'rougeL': 0.0}},\n",
       " 'evil + llm_tree_idx': {'0': {'faithfulness': 0.771,\n",
       "   'answer_relevancy': 0.087,\n",
       "   'context_recall': 0.719,\n",
       "   'context_precision': 0.854,\n",
       "   'bleu': 0.017,\n",
       "   'rouge1': 0.222,\n",
       "   'rougeL': 0.222},\n",
       "  '1': {'faithfulness': 0.84,\n",
       "   'answer_relevancy': 0.152,\n",
       "   'context_recall': 0.63,\n",
       "   'context_precision': 0.817,\n",
       "   'bleu': 0.008,\n",
       "   'rouge1': 0.154,\n",
       "   'rougeL': 0.154},\n",
       "  'mean': {'faithfulness': 0.80550003,\n",
       "   'answer_relevancy': 0.1195,\n",
       "   'context_recall': 0.6745,\n",
       "   'context_precision': 0.8355,\n",
       "   'bleu': 0.0125,\n",
       "   'rouge1': 0.188,\n",
       "   'rougeL': 0.188}},\n",
       " 'qwen-2-72b + inverted_idx': {'0': {'faithfulness': 0.776,\n",
       "   'answer_relevancy': 0.085,\n",
       "   'context_recall': 0.719,\n",
       "   'context_precision': 0.732,\n",
       "   'bleu': 0.008,\n",
       "   'rouge1': 0.0,\n",
       "   'rougeL': 0.0},\n",
       "  '1': {'faithfulness': 0.636,\n",
       "   'answer_relevancy': 0.108,\n",
       "   'context_recall': 0.446,\n",
       "   'context_precision': 0.636,\n",
       "   'bleu': 0,\n",
       "   'rouge1': 0.0,\n",
       "   'rougeL': 0.0},\n",
       "  'mean': {'faithfulness': 0.706,\n",
       "   'answer_relevancy': 0.0965,\n",
       "   'context_recall': 0.5825,\n",
       "   'context_precision': 0.684,\n",
       "   'bleu': 0.004,\n",
       "   'rouge1': 0.0,\n",
       "   'rougeL': 0.0}},\n",
       " 'qwen-2-72b + llm_tree_idx': {'0': {'faithfulness': 0.77,\n",
       "   'answer_relevancy': 0.066,\n",
       "   'context_recall': 0.719,\n",
       "   'context_precision': 0.739,\n",
       "   'bleu': 0.011,\n",
       "   'rouge1': 0.194,\n",
       "   'rougeL': 0.194},\n",
       "  '1': {'faithfulness': 0.757,\n",
       "   'answer_relevancy': 0.137,\n",
       "   'context_recall': 0.63,\n",
       "   'context_precision': 0.838,\n",
       "   'bleu': 0,\n",
       "   'rouge1': 0.0,\n",
       "   'rougeL': 0.0},\n",
       "  'mean': {'faithfulness': 0.7635,\n",
       "   'answer_relevancy': 0.1015,\n",
       "   'context_recall': 0.6745,\n",
       "   'context_precision': 0.7885,\n",
       "   'bleu': 0.0055,\n",
       "   'rouge1': 0.097,\n",
       "   'rougeL': 0.097}}}"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_llm_metrics(\n",
    "    llm_metrics_path: Path = LLM_METRICS_PATH,\n",
    "    llm_res_path: Path = LLM_RES_PATH,\n",
    "    force: bool = False,\n",
    ") -> dict:\n",
    "    model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "    llm_responses = load_json(llm_res_path)\n",
    "\n",
    "    if force:\n",
    "        remove_path(llm_metrics_path)\n",
    "\n",
    "    llm_metrics = load_json(llm_metrics_path, allow_empty=True)\n",
    "\n",
    "    for model_name, queries in llm_responses.items():\n",
    "        for query_name, query_res in tqdm(queries.items(), desc=model_name):\n",
    "            if model_name not in llm_metrics:\n",
    "                llm_metrics[model_name] = {}\n",
    "            if query_name in llm_metrics[model_name]:\n",
    "                continue\n",
    "            llm_metrics[model_name][query_name] = compute_query_llm_metrics(\n",
    "                query_res, get_embedding=lambda x: model.encode([x])[0]\n",
    "            )\n",
    "        # Compute mean\n",
    "        llm_metrics[model_name][\"mean\"] = {\n",
    "            k: np.mean(\n",
    "                [v[k] for v in llm_metrics[model_name].values() if isinstance(v, dict)]\n",
    "            )\n",
    "            for k in list(llm_metrics[model_name].values())[0].keys()\n",
    "        }\n",
    "\n",
    "    save_json(llm_metrics_path, llm_metrics)\n",
    "\n",
    "    return llm_metrics\n",
    "\n",
    "\n",
    "general_metrics = compute_llm_metrics(force=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_at_k(ground_truth, predicted, k):\n",
    "    \"\"\"Compute Precision@k\"\"\"\n",
    "    if k == 0:\n",
    "        return 0.0\n",
    "    predicted = predicted[:k]\n",
    "    relevant = set(ground_truth)\n",
    "    hits = sum(1 for item in predicted if item in relevant)\n",
    "    return hits / k\n",
    "\n",
    "\n",
    "def recall_at_k(ground_truth, predicted, k):\n",
    "    \"\"\"Compute Recall@k\"\"\"\n",
    "    if not ground_truth:\n",
    "        return 0.0\n",
    "    predicted = predicted[:k]\n",
    "    relevant = set(ground_truth)\n",
    "    hits = sum(1 for item in predicted if item in relevant)\n",
    "    return hits / len(ground_truth)\n",
    "\n",
    "\n",
    "def ap_at_k(ground_truth, predicted, k):\n",
    "    \"\"\"Compute Average Precision@k\"\"\"\n",
    "    if not ground_truth or k == 0:\n",
    "        return 0.0\n",
    "\n",
    "    predicted = predicted[:k]\n",
    "    relevant = set(ground_truth)\n",
    "    hits = 0\n",
    "    sum_precisions = 0.0\n",
    "\n",
    "    for i, item in enumerate(predicted, 1):\n",
    "        if item in relevant:\n",
    "            hits += 1\n",
    "            sum_precisions += hits / i\n",
    "\n",
    "    return sum_precisions / min(len(ground_truth), k)\n",
    "\n",
    "\n",
    "def map_at_k(ground_truth_list, predicted_list, k):\n",
    "    \"\"\"Compute Mean Average Precision@k across multiple queries\"\"\"\n",
    "    return np.mean(\n",
    "        [ap_at_k(gt, pred, k) for gt, pred in zip(ground_truth_list, predicted_list)]\n",
    "    )\n",
    "\n",
    "\n",
    "def recall_rate_at_k(ground_truth_list, predicted_list, k):\n",
    "    \"\"\"Compute Mean Recall Rate@k across multiple queries\"\"\"\n",
    "    return np.mean(\n",
    "        [\n",
    "            recall_at_k(gt, pred[:k], k)\n",
    "            for gt, pred in zip(ground_truth_list, predicted_list)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "def dcg_at_k(ground_truth, predicted, k):\n",
    "    \"\"\"Compute Discounted Cumulative Gain@k\"\"\"\n",
    "    predicted = predicted[:k]\n",
    "    relevant = set(ground_truth)\n",
    "    gains = [\n",
    "        1.0 / np.log2(i + 2) if item in relevant else 0.0\n",
    "        for i, item in enumerate(predicted)\n",
    "    ]\n",
    "    return sum(gains)\n",
    "\n",
    "\n",
    "def ndcg_at_k(ground_truth, predicted, k):\n",
    "    \"\"\"Compute Normalized DCG@k\"\"\"\n",
    "    idcg = dcg_at_k(ground_truth, ground_truth, k)\n",
    "    if idcg == 0:\n",
    "        return 0.0\n",
    "    dcg = dcg_at_k(ground_truth, predicted, k)\n",
    "    return dcg / idcg\n",
    "\n",
    "\n",
    "def mean_ndcg_at_k(ground_truth_list, predicted_list, k):\n",
    "    \"\"\"Compute Mean nDCG@k across multiple queries\"\"\"\n",
    "    return np.mean(\n",
    "        [ndcg_at_k(gt, pred, k) for gt, pred in zip(ground_truth_list, predicted_list)]\n",
    "    )\n",
    "\n",
    "\n",
    "def mrr_at_k(ground_truth_list, predicted_list, k):\n",
    "    \"\"\"Compute Mean Reciprocal Rank@k across multiple queries\"\"\"\n",
    "    rr_scores = []\n",
    "    for gt, pred in zip(ground_truth_list, predicted_list):\n",
    "        pred = pred[:k]\n",
    "        relevant = set(gt)\n",
    "        for i, item in enumerate(pred, 1):\n",
    "            if item in relevant:\n",
    "                rr_scores.append(1.0 / i)\n",
    "                break\n",
    "        else:\n",
    "            rr_scores.append(0.0)\n",
    "    return np.mean(rr_scores)\n",
    "\n",
    "\n",
    "def f1_at_k(ground_truth_list, predicted_list, k):\n",
    "    \"\"\"Compute F1-score@k across multiple queries\"\"\"\n",
    "    f1_scores = []\n",
    "    for gt, pred in zip(ground_truth_list, predicted_list):\n",
    "        p = precision_at_k(gt, pred, k)\n",
    "        r = recall_at_k(gt, pred, k)\n",
    "        if (p + r) == 0:\n",
    "            f1_scores.append(0.0)\n",
    "        else:\n",
    "            f1_scores.append(2 * (p * r) / (p + r))\n",
    "    return np.mean(f1_scores)\n",
    "\n",
    "\n",
    "def compute_query_general_metrics(\n",
    "    ground_truth_list: list[list[int]], results_list: list[list[int]], k_values: list[int]\n",
    ") -> dict:\n",
    "    results = {\n",
    "        str(k): {\n",
    "            \"MAP\": map_at_k(ground_truth_list, results_list, k),\n",
    "            \"MAR\": recall_rate_at_k(ground_truth_list, results_list, k),\n",
    "            \"nDCG\": mean_ndcg_at_k(ground_truth_list, results_list, k),\n",
    "            \"MRR\": mrr_at_k(ground_truth_list, results_list, k),\n",
    "            \"F1\": f1_at_k(ground_truth_list, results_list, k),\n",
    "        }\n",
    "        for k in k_values\n",
    "    }\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "evil + inverted_idx: 100%|██████████| 2/2 [00:00<00:00, 16163.02it/s]\n",
      "evil + llm_tree_idx: 100%|██████████| 2/2 [00:00<00:00, 41527.76it/s]\n",
      "qwen-2-72b + inverted_idx: 100%|██████████| 2/2 [00:00<00:00, 48770.98it/s]\n",
      "qwen-2-72b + llm_tree_idx: 100%|██████████| 2/2 [00:00<00:00, 55924.05it/s]\n",
      "inverted_idx: 100%|██████████| 2/2 [00:00<00:00, 52428.80it/s]\n",
      "llm_tree_idx: 100%|██████████| 2/2 [00:00<00:00, 44384.17it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'evil + inverted_idx': {'1': {'MAP': 0.5,\n",
       "   'MAR': 0.25,\n",
       "   'nDCG': 0.5,\n",
       "   'MRR': 0.5,\n",
       "   'F1': 0.3333333333333333},\n",
       "  '3': {'MAP': 0.5, 'MAR': 0.5, 'nDCG': 0.5, 'MRR': 0.5, 'F1': 0.4},\n",
       "  '5': {'MAP': 0.5,\n",
       "   'MAR': 0.5,\n",
       "   'nDCG': 0.5,\n",
       "   'MRR': 0.5,\n",
       "   'F1': 0.28571428571428575},\n",
       "  '10': {'MAP': 0.5,\n",
       "   'MAR': 0.5,\n",
       "   'nDCG': 0.5,\n",
       "   'MRR': 0.5,\n",
       "   'F1': 0.16666666666666669}},\n",
       " 'evil + llm_tree_idx': {'1': {'MAP': 1.0,\n",
       "   'MAR': 0.75,\n",
       "   'nDCG': 1.0,\n",
       "   'MRR': 1.0,\n",
       "   'F1': 0.8333333333333333},\n",
       "  '3': {'MAP': 1.0, 'MAR': 1.0, 'nDCG': 1.0, 'MRR': 1.0, 'F1': 0.65},\n",
       "  '5': {'MAP': 1.0,\n",
       "   'MAR': 1.0,\n",
       "   'nDCG': 1.0,\n",
       "   'MRR': 1.0,\n",
       "   'F1': 0.45238095238095244},\n",
       "  '10': {'MAP': 1.0,\n",
       "   'MAR': 1.0,\n",
       "   'nDCG': 1.0,\n",
       "   'MRR': 1.0,\n",
       "   'F1': 0.25757575757575757}},\n",
       " 'qwen-2-72b + inverted_idx': {'1': {'MAP': 0.5,\n",
       "   'MAR': 0.25,\n",
       "   'nDCG': 0.5,\n",
       "   'MRR': 0.5,\n",
       "   'F1': 0.3333333333333333},\n",
       "  '3': {'MAP': 0.5, 'MAR': 0.5, 'nDCG': 0.5, 'MRR': 0.5, 'F1': 0.4},\n",
       "  '5': {'MAP': 0.5,\n",
       "   'MAR': 0.5,\n",
       "   'nDCG': 0.5,\n",
       "   'MRR': 0.5,\n",
       "   'F1': 0.28571428571428575},\n",
       "  '10': {'MAP': 0.5,\n",
       "   'MAR': 0.5,\n",
       "   'nDCG': 0.5,\n",
       "   'MRR': 0.5,\n",
       "   'F1': 0.16666666666666669}},\n",
       " 'qwen-2-72b + llm_tree_idx': {'1': {'MAP': 1.0,\n",
       "   'MAR': 0.75,\n",
       "   'nDCG': 1.0,\n",
       "   'MRR': 1.0,\n",
       "   'F1': 0.8333333333333333},\n",
       "  '3': {'MAP': 1.0, 'MAR': 1.0, 'nDCG': 1.0, 'MRR': 1.0, 'F1': 0.65},\n",
       "  '5': {'MAP': 1.0,\n",
       "   'MAR': 1.0,\n",
       "   'nDCG': 1.0,\n",
       "   'MRR': 1.0,\n",
       "   'F1': 0.45238095238095244},\n",
       "  '10': {'MAP': 1.0,\n",
       "   'MAR': 1.0,\n",
       "   'nDCG': 1.0,\n",
       "   'MRR': 1.0,\n",
       "   'F1': 0.25757575757575757}},\n",
       " 'inverted_idx': {'1': {'MAP': 0.5,\n",
       "   'MAR': 0.25,\n",
       "   'nDCG': 0.5,\n",
       "   'MRR': 0.5,\n",
       "   'F1': 0.3333333333333333},\n",
       "  '3': {'MAP': 0.5, 'MAR': 0.5, 'nDCG': 0.5, 'MRR': 0.5, 'F1': 0.4},\n",
       "  '5': {'MAP': 0.5,\n",
       "   'MAR': 0.5,\n",
       "   'nDCG': 0.5,\n",
       "   'MRR': 0.5,\n",
       "   'F1': 0.28571428571428575},\n",
       "  '10': {'MAP': 0.5,\n",
       "   'MAR': 0.5,\n",
       "   'nDCG': 0.5,\n",
       "   'MRR': 0.5,\n",
       "   'F1': 0.16666666666666669}},\n",
       " 'llm_tree_idx': {'1': {'MAP': 1.0,\n",
       "   'MAR': 0.75,\n",
       "   'nDCG': 1.0,\n",
       "   'MRR': 1.0,\n",
       "   'F1': 0.8333333333333333},\n",
       "  '3': {'MAP': 1.0, 'MAR': 1.0, 'nDCG': 1.0, 'MRR': 1.0, 'F1': 0.65},\n",
       "  '5': {'MAP': 1.0,\n",
       "   'MAR': 1.0,\n",
       "   'nDCG': 1.0,\n",
       "   'MRR': 1.0,\n",
       "   'F1': 0.45238095238095244},\n",
       "  '10': {'MAP': 1.0,\n",
       "   'MAR': 1.0,\n",
       "   'nDCG': 1.0,\n",
       "   'MRR': 1.0,\n",
       "   'F1': 0.25757575757575757}}}"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_general_metrics(\n",
    "    general_metrics_path: Path = GENERAl_METRICS_PATH,\n",
    "    llm_res_path: Path = LLM_RES_PATH,\n",
    "    indexer_res_path: Path = INDEXER_RES_PATH,\n",
    "    k_values: list[int] = [1, 3, 5, 10],\n",
    "    force: bool = False,\n",
    ") -> dict:\n",
    "    llm_responses = load_json(llm_res_path)\n",
    "    indexer_responses = load_json(indexer_res_path)\n",
    "\n",
    "    if force:\n",
    "        remove_path(general_metrics_path)\n",
    "\n",
    "    general_metrics = load_json(general_metrics_path, allow_empty=True)\n",
    "\n",
    "    for model_name, queries in [*llm_responses.items(), *indexer_responses.items()]:\n",
    "        ground_truth_list, responses_list = [], []\n",
    "        for query_name, query_res in tqdm(queries.items(), desc=model_name):\n",
    "            if model_name not in general_metrics:\n",
    "                general_metrics[model_name] = {}\n",
    "\n",
    "            ground_truth_list.append(query_res[\"ground_truths_idx\"])\n",
    "            responses_list.append(query_res[\"responses_idx\"])\n",
    "        general_metrics[model_name] = compute_query_general_metrics(\n",
    "            ground_truth_list, responses_list, k_values\n",
    "        )\n",
    "\n",
    "    save_json(general_metrics_path, general_metrics)\n",
    "\n",
    "    return general_metrics\n",
    "\n",
    "\n",
    "general_metrics = compute_general_metrics(force=True)\n",
    "\n",
    "general_metrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
