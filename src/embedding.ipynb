{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading stopwords: <urlopen error [WinError 10061]\n",
      "[nltk_data]     Подключение не установлено, т.к. конечный компьютер\n",
      "[nltk_data]     отверг запрос на подключение>\n",
      "[nltk_data] Error loading punkt_tab: <urlopen error [WinError 10061]\n",
      "[nltk_data]     Подключение не установлено, т.к. конечный компьютер\n",
      "[nltk_data]     отверг запрос на подключение>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "from collections import Counter\n",
    "\n",
    "import nltk\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.similarities.annoy import AnnoyIndexer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from utils import from_current_file, load_json, round_float, save_json\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"punkt_tab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index is not found, creating new...\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Annoy not installed. To use the Annoy indexer, please run `pip install annoy`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Kiaver\\PycharmProjects\\ir-s25\\.venv\\Lib\\site-packages\\gensim\\similarities\\annoy.py:156\u001b[0m, in \u001b[0;36mAnnoyIndexer._build_from_model\u001b[1;34m(self, vectors, labels, num_features)\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 156\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mannoy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AnnoyIndex\n\u001b[0;32m    157\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'annoy'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 139\u001b[0m\n\u001b[0;32m    131\u001b[0m             results\u001b[38;5;241m.\u001b[39mappend((doc_id, cosine_sim))\n\u001b[0;32m    133\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m    134\u001b[0m             (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdocuments[doc_id], round_float(score, \u001b[38;5;241m5\u001b[39m))\n\u001b[0;32m    135\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m doc_id, score \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(results, key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;241m-\u001b[39mx[\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m    136\u001b[0m         ]\n\u001b[1;32m--> 139\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[43mWord2VecIndexer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    140\u001b[0m results \u001b[38;5;241m=\u001b[39m indexer\u001b[38;5;241m.\u001b[39mfind_semantic(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msin\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    141\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc, score \u001b[38;5;129;01min\u001b[39;00m results:\n",
      "Cell \u001b[1;32mIn[6], line 31\u001b[0m, in \u001b[0;36mWord2VecIndexer.__init__\u001b[1;34m(self, index_dir, documents_dir, top_similar, force)\u001b[0m\n\u001b[0;32m     29\u001b[0m             \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m     30\u001b[0m     os\u001b[38;5;241m.\u001b[39mmkdir(path\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_dir)\n\u001b[1;32m---> 31\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComplete!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_index()\n",
      "Cell \u001b[1;32mIn[6], line 69\u001b[0m, in \u001b[0;36mWord2VecIndexer.build_index\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m Word2Vec(\n\u001b[0;32m     62\u001b[0m     sentences\u001b[38;5;241m=\u001b[39msentences,\n\u001b[0;32m     63\u001b[0m     vector_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     66\u001b[0m     workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m,\n\u001b[0;32m     67\u001b[0m )\n\u001b[0;32m     68\u001b[0m \u001b[38;5;66;03m# Initialize Annoy indexer\u001b[39;00m\n\u001b[1;32m---> 69\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mannoy_indexer \u001b[38;5;241m=\u001b[39m \u001b[43mAnnoyIndexer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_trees\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;66;03m# Persist model and index\u001b[39;00m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_word2vec_model_path)\n",
      "File \u001b[1;32mc:\\Users\\Kiaver\\PycharmProjects\\ir-s25\\.venv\\Lib\\site-packages\\gensim\\similarities\\annoy.py:85\u001b[0m, in \u001b[0;36mAnnoyIndexer.__init__\u001b[1;34m(self, model, num_trees)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     84\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOnly a Word2Vec, Doc2Vec, FastText or KeyedVectors instance can be used\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 85\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_build_from_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_normed_vectors\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex_to_key\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvector_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Kiaver\\PycharmProjects\\ir-s25\\.venv\\Lib\\site-packages\\gensim\\similarities\\annoy.py:158\u001b[0m, in \u001b[0;36mAnnoyIndexer._build_from_model\u001b[1;34m(self, vectors, labels, num_features)\u001b[0m\n\u001b[0;32m    156\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mannoy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AnnoyIndex\n\u001b[0;32m    157\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[1;32m--> 158\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m _NOANNOY\n\u001b[0;32m    160\u001b[0m index \u001b[38;5;241m=\u001b[39m AnnoyIndex(num_features, metric\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mangular\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m vector_num, vector \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(vectors):\n",
      "\u001b[1;31mImportError\u001b[0m: Annoy not installed. To use the Annoy indexer, please run `pip install annoy`."
     ]
    }
   ],
   "source": [
    "class Word2VecIndexer:\n",
    "    _stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        index_dir: str = \"../data/embedding_directory\",\n",
    "        documents_dir: str = \"../data/scrapped/class_data_function__1_1\",\n",
    "        top_similar: int = 10,\n",
    "        force: bool = False,\n",
    "    ):\n",
    "        self._index_dir = from_current_file(index_dir)\n",
    "        self._documents_dir = from_current_file(documents_dir)\n",
    "        self.top_similar = top_similar\n",
    "\n",
    "        self._word2vec_model_path = os.path.join(self._index_dir, \"word2vec.model\")\n",
    "        self._annoy_index_path = os.path.join(self._index_dir, \"annoy_index\")\n",
    "        self._doc_id_path = os.path.join(self._index_dir, \"documents.json\")\n",
    "        self.documents: dict[int, str] = {}\n",
    "\n",
    "        self.model: Word2Vec = None\n",
    "        self.annoy_indexer: AnnoyIndexer = None\n",
    "\n",
    "        if force or not os.path.exists(self._index_dir):\n",
    "            print(\"Index is not found, creating new...\")\n",
    "            if force:\n",
    "                try:\n",
    "                    shutil.rmtree(self._index_dir)\n",
    "                except FileNotFoundError:\n",
    "                    pass\n",
    "            os.mkdir(path=self._index_dir)\n",
    "            self.build_index()\n",
    "            print(\"Complete!\")\n",
    "\n",
    "        self.load_index()\n",
    "\n",
    "    def _tokenize(self, text: str) -> list[str]:\n",
    "        return [w for w in re.findall(r\"\\w+\", text.lower()) if w not in self._stop_words]\n",
    "\n",
    "    def _get_similar_words(self, word: str) -> set[tuple[str, float]]:\n",
    "        matches = set()\n",
    "        if self.model and word in self.model.wv:\n",
    "            for similar_word, similarity in self.model.wv.most_similar(\n",
    "                word, topn=self.top_similar, indexer=self.annoy_indexer\n",
    "            ):\n",
    "                if similar_word in self.index:\n",
    "                    matches.add((similar_word, similarity))\n",
    "        return matches\n",
    "\n",
    "    def build_index(self):\n",
    "        sentences = []\n",
    "        for document_id, filename in enumerate(os.listdir(self._documents_dir)):\n",
    "            if filename.endswith(\".txt\"):\n",
    "                with open(\n",
    "                    os.path.join(self._documents_dir, filename), \"r\", encoding=\"utf-8\"\n",
    "                ) as f:\n",
    "                    text = f.read()\n",
    "                    self.documents[document_id] = filename[:-4]\n",
    "                    words = self._tokenize(text)\n",
    "                    sentences.append(words)\n",
    "\n",
    "        self.model = Word2Vec(\n",
    "            sentences=sentences,\n",
    "            vector_size=100,\n",
    "            window=5,\n",
    "            min_count=1,\n",
    "            workers=4,\n",
    "        )\n",
    "        # Initialize Annoy indexer\n",
    "        self.annoy_indexer = AnnoyIndexer(self.model, num_trees=100)\n",
    "\n",
    "        # Persist model and index\n",
    "        self.model.save(self._word2vec_model_path)\n",
    "        self.annoy_indexer.save(self._annoy_index_path)\n",
    "\n",
    "        save_json(self._doc_id_path, self.documents)\n",
    "\n",
    "    def load_index(self):\n",
    "        self.documents = {int(k): v for k, v in load_json(self._doc_id_path).items()}\n",
    "        self.model = Word2Vec.load(self._word2vec_model_path)\n",
    "        self.annoy_indexer = AnnoyIndexer()\n",
    "        self.annoy_indexer.load(self._annoy_index_path)\n",
    "        self.annoy_indexer.model = self.model\n",
    "\n",
    "    def find(self, query: str, top_k: int = 10) -> list:\n",
    "        query_words = self._tokenize(query)\n",
    "        document_scores = Counter()\n",
    "        total_documents = len(self.documents)\n",
    "\n",
    "        for word in query_words:\n",
    "            matching_words = self._get_similar_words(word) | {(word, 1)}\n",
    "\n",
    "            for match, distance_coef in matching_words:\n",
    "                if match in self.index:\n",
    "                    doc_freq = len(self.index[match])\n",
    "                    idf = math.log(total_documents / (1 + doc_freq))\n",
    "\n",
    "                    for doc_id in self.index[match]:\n",
    "                        tf = (\n",
    "                            self.document_word_count[doc_id][match]\n",
    "                            / self.document_lengths[doc_id]\n",
    "                        )\n",
    "                        document_scores[doc_id] += tf * idf * distance_coef  # type: ignore\n",
    "\n",
    "        ranked_docs = sorted(document_scores.items(), key=lambda x: -x[1])[:top_k]\n",
    "        return [\n",
    "            (self.documents[doc_id], round_float(score, 5))\n",
    "            for doc_id, score in ranked_docs\n",
    "        ]\n",
    "\n",
    "    def find_semantic(self, query: str, top_k: int = 10) -> list:\n",
    "        query_words = self._tokenize(query)\n",
    "        query_vectors = []\n",
    "        for word in query_words:\n",
    "            if word in self.model.wv:\n",
    "                query_vectors.append(self.model.wv[word])\n",
    "\n",
    "        if not query_vectors:\n",
    "            return []\n",
    "\n",
    "        query_embedding = sum(query_vectors) / len(query_vectors)\n",
    "\n",
    "        doc_ids, distances = self.annoy_indexer.get_nns_by_vector(\n",
    "            query_embedding, top_k, include_distances=True\n",
    "        )\n",
    "\n",
    "        results = []\n",
    "        for doc_id, distance in zip(doc_ids, distances):\n",
    "            cosine_sim = (\n",
    "                1 - (distance**2) / 2\n",
    "            )  # Convert angular distance to cosine similarity\n",
    "            results.append((doc_id, cosine_sim))\n",
    "\n",
    "        return [\n",
    "            (self.documents[doc_id], round_float(score, 5))\n",
    "            for doc_id, score in sorted(results, key=lambda x: -x[1])\n",
    "        ]\n",
    "\n",
    "\n",
    "indexer = Word2VecIndexer()\n",
    "results = indexer.find_semantic(\"sin\")\n",
    "for doc, score in results:\n",
    "    print(f\"Score: {score}\\tFile: {doc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
