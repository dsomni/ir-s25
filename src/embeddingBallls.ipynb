{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kiaver\\PycharmProjects\\ir-s25\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: computer understanding of human language\n",
      "Top 3 results:\n",
      "- Natural language processing helps computers understand text (distance: 0.7042)\n",
      "- Machine learning requires large amounts of data (distance: 0.9758)\n",
      "- Artificial intelligence is transforming industries (distance: 1.0927)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.neighbors import BallTree\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "class BERTEmbedder:\n",
    "    def __init__(self, model_name=\"sentence-transformers/stsb-bert-base\", device=None):\n",
    "        self.device = device or 'cpu'\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModel.from_pretrained(model_name).to(self.device)\n",
    "        self.model.eval()\n",
    "        \n",
    "        self.do_lower_case = getattr(self.tokenizer, 'do_lower_case', False)\n",
    "\n",
    "    def text_to_embedding(self, texts, pooling='mean', normalize=False):\n",
    "        is_single = isinstance(texts, str)\n",
    "        texts = [texts] if is_single else texts\n",
    "        \n",
    "        inputs = self.tokenizer(\n",
    "            texts,\n",
    "            return_tensors='pt',\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=128\n",
    "        ).to(self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "            \n",
    "        if pooling == 'mean':\n",
    "            mask = inputs['attention_mask'].unsqueeze(-1)\n",
    "            embeddings = (outputs.last_hidden_state * mask).sum(1) / mask.sum(1).clamp(min=1e-9)\n",
    "        elif pooling == 'cls':\n",
    "            embeddings = outputs.last_hidden_state[:, 0, :]\n",
    "        else:\n",
    "            raise ValueError(\"Invalid pooling method\")\n",
    "            \n",
    "        if normalize:\n",
    "            embeddings = torch.nn.functional.normalize(embeddings, p=2, dim=1)\n",
    "            \n",
    "        return embeddings.cpu().numpy()[0] if is_single else embeddings.cpu().numpy()\n",
    "\n",
    "\n",
    "class BERTBallTree:\n",
    "    def __init__(self, embedder=None, metric='euclidean', leaf_size=40):\n",
    "        \"\"\"\n",
    "        Initialize the BERT Ball Tree.\n",
    "        \n",
    "        Args:\n",
    "            embedder: Pre-initialized BERTEmbedder instance\n",
    "            metric: Distance metric for BallTree ('euclidean', 'cosine', etc.)\n",
    "            leaf_size: Affects the speed of queries and memory usage\n",
    "        \"\"\"\n",
    "        self.embedder = embedder or BERTEmbedder()\n",
    "        self.metric = metric\n",
    "        self.leaf_size = leaf_size\n",
    "        self.tree = None\n",
    "        self.texts = None\n",
    "    \n",
    "    def build_tree(self, texts):\n",
    "        \"\"\"\n",
    "        Build the Ball Tree from a list of texts.\n",
    "        \n",
    "        Args:\n",
    "            texts: List of strings to index\n",
    "        \"\"\"\n",
    "        self.texts = np.array(texts)  # Store the original texts\n",
    "        embeddings = self.embedder.text_to_embedding(texts, pooling='mean', normalize=True)\n",
    "        self.tree = BallTree(embeddings, metric=self.metric, leaf_size=self.leaf_size)\n",
    "    \n",
    "    def query(self, query_text, k=5, return_distances=False):\n",
    "        \"\"\"\n",
    "        Query the Ball Tree for nearest neighbors.\n",
    "        \n",
    "        Args:\n",
    "            query_text: The query text string\n",
    "            k: Number of nearest neighbors to return\n",
    "            return_distances: Whether to return distances along with results\n",
    "            \n",
    "        Returns:\n",
    "            If return_distances is False: list of nearest texts\n",
    "            If return_distances is True: tuple of (texts, distances)\n",
    "        \"\"\"\n",
    "        if self.tree is None:\n",
    "            raise ValueError(\"Ball Tree has not been built yet. Call build_tree() first.\")\n",
    "            \n",
    "        # Get embedding for the query text\n",
    "        query_embedding = self.embedder.text_to_embedding(\n",
    "            query_text, pooling='mean', normalize=True\n",
    "        ).reshape(1, -1)\n",
    "        \n",
    "        # Query the tree\n",
    "        distances, indices = self.tree.query(query_embedding, k=k)\n",
    "        \n",
    "        # Get the corresponding texts\n",
    "        results = self.texts[indices[0]]\n",
    "        \n",
    "        if return_distances:\n",
    "            return results, distances[0]\n",
    "        return results\n",
    "    \n",
    "    def save_tree(self, filepath):\n",
    "        \"\"\"Save the Ball Tree and associated data to disk.\"\"\"\n",
    "        import joblib\n",
    "        data = {\n",
    "            'texts': self.texts,\n",
    "            'tree': self.tree,\n",
    "            'metric': self.metric,\n",
    "            'leaf_size': self.leaf_size\n",
    "        }\n",
    "        joblib.dump(data, filepath)\n",
    "    \n",
    "    @classmethod\n",
    "    def load_tree(cls, filepath, embedder=None):\n",
    "        \"\"\"Load a saved Ball Tree from disk.\"\"\"\n",
    "        import joblib\n",
    "        data = joblib.load(filepath)\n",
    "        instance = cls(embedder=embedder, metric=data['metric'], leaf_size=data['leaf_size'])\n",
    "        instance.texts = data['texts']\n",
    "        instance.tree = data['tree']\n",
    "        return instance\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Sample texts\n",
    "    texts = [\n",
    "        \"The quick brown fox jumps over the lazy dog\",\n",
    "        \"Artificial intelligence is transforming industries\",\n",
    "        \"Python is a popular programming language\",\n",
    "        \"Machine learning requires large amounts of data\",\n",
    "        \"Deep learning models use neural networks\",\n",
    "        \"Natural language processing helps computers understand text\",\n",
    "        \"The weather is nice today\",\n",
    "        \"I enjoy reading books in my free time\"\n",
    "    ]\n",
    "    \n",
    "    # Initialize and build the tree\n",
    "    ball_tree = BERTBallTree()\n",
    "    ball_tree.build_tree(texts)\n",
    "    \n",
    "    # Query the tree\n",
    "    query = \"computer understanding of human language\"\n",
    "    results, distances = ball_tree.query(query, k=3, return_distances=True)\n",
    "    \n",
    "    print(f\"Query: {query}\")\n",
    "    print(\"Top 3 results:\")\n",
    "    for text, dist in zip(results, distances):\n",
    "        print(f\"- {text} (distance: {dist:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Kiaver\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Kiaver\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import nltk\n",
    "from gensim.models import Word2Vec\n",
    "from annoy import AnnoyIndex\n",
    "from nltk.corpus import stopwords\n",
    "from tqdm import tqdm\n",
    "\n",
    "from utils import from_current_file, load_json, round_float, save_json\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"punkt_tab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.99148\tFile: math.tau\n",
      "Score: 0.98984\tFile: csv.QUOTE_MINIMAL\n",
      "Score: 0.9872\tFile: mimetypes.guess_extension\n",
      "Score: 0.98673\tFile: io.BytesIO.getvalue\n",
      "Score: 0.98517\tFile: urllib.parse.SplitResultBytes\n",
      "Score: 0.98508\tFile: math.trunc\n",
      "Score: 0.98434\tFile: ast.Interactive\n",
      "Score: 0.98432\tFile: subprocess.STARTF_USESTDHANDLES\n",
      "Score: 0.98393\tFile: os.sysconf_names\n",
      "Score: 0.98296\tFile: functools.singledispatchmethod\n"
     ]
    }
   ],
   "source": [
    "class Word2VecIndexer:\n",
    "    _stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        index_dir: str = \"../data/embedding_directory\",\n",
    "        documents_dir: str = \"../data/scrapped/class_data_function__1_1\",\n",
    "        top_similar: int = 10,\n",
    "        force: bool = False,\n",
    "    ):\n",
    "        self._index_dir = from_current_file(index_dir)\n",
    "        self._documents_dir = from_current_file(documents_dir)\n",
    "        self.top_similar = top_similar\n",
    "\n",
    "        self._word2vec_model_path = os.path.join(self._index_dir, \"word2vec.model\")\n",
    "        self._annoy_index_path = os.path.join(self._index_dir, \"doc_embeddings.ann\")\n",
    "        self.doc_embeddings: dict[int, np.ndarray] = {}  # Document ID -> embedding\n",
    "        self.annoy_index: AnnoyIndex = None  # Annoy index for document embeddings\n",
    "        self._doc_id_path = os.path.join(self._index_dir, \"documents.json\")\n",
    "        self.documents: dict[int, str] = {}\n",
    "\n",
    "        self.model: Word2Vec = None\n",
    "\n",
    "        if force or not os.path.exists(self._index_dir):\n",
    "            print(\"Index is not found, creating new...\")\n",
    "            if force:\n",
    "                try:\n",
    "                    shutil.rmtree(self._index_dir)\n",
    "                except FileNotFoundError:\n",
    "                    pass\n",
    "            os.mkdir(path=self._index_dir)\n",
    "            self.build_index()\n",
    "            print(\"Complete!\")\n",
    "\n",
    "        self.load_index()\n",
    "\n",
    "    def _tokenize(self, text: str) -> list[str]:\n",
    "        return [w for w in re.findall(r\"\\w+\", text.lower()) if w not in self._stop_words]\n",
    "\n",
    "    def _get_similar_words(self, word: str) -> set[tuple[str, float]]:\n",
    "        matches = set()\n",
    "        if self.model and word in self.model.wv:\n",
    "            for similar_word, similarity in self.model.wv.most_similar(\n",
    "                word, topn=self.top_similar, indexer=self.annoy_indexer\n",
    "            ):\n",
    "                if similar_word in self.index:\n",
    "                    matches.add((similar_word, similarity))\n",
    "        return matches\n",
    "\n",
    "    def build_index(self):\n",
    "        sentences = []\n",
    "        for document_id, filename in enumerate(os.listdir(self._documents_dir)):\n",
    "            if filename.endswith(\".txt\"):\n",
    "                with open(\n",
    "                    os.path.join(self._documents_dir, filename), \"r\", encoding=\"utf-8\"\n",
    "                ) as f:\n",
    "                    text = f.read()\n",
    "                    self.documents[document_id] = filename[:-4]\n",
    "                    words = self._tokenize(text)\n",
    "                    sentences.append(words)\n",
    "\n",
    "        self.model = Word2Vec(\n",
    "            sentences=sentences,\n",
    "            min_count = 1,\n",
    "        )\n",
    "        vector_size = self.model.vector_size\n",
    "\n",
    "        self.doc_embeddings = {\n",
    "            doc_id: np.mean([\n",
    "                self.model.wv[word] \n",
    "                for word in words \n",
    "                if word in self.model.wv\n",
    "            ], axis=0) \n",
    "            for doc_id, words in enumerate(sentences)\n",
    "        }\n",
    "\n",
    "        # self.doc_embeddings = {}\n",
    "        # for doc_id, words in tqdm(enumerate(sentences)):\n",
    "        #     self.doc_embeddings[doc_id] = np.mean([\n",
    "        #         self.model.wv[word] \n",
    "        #         for word in words \n",
    "        #         if word in self.model.wv\n",
    "        #     ], axis=0) \n",
    "\n",
    "        # Build Annoy index for documents\n",
    "        self.annoy_index = AnnoyIndex(vector_size, 'angular')\n",
    "        for doc_id, embedding in self.doc_embeddings.items():\n",
    "            self.annoy_index.add_item(doc_id, embedding)\n",
    "        self.annoy_index.build(n_trees=1000)\n",
    "        self.annoy_index.save(self._annoy_index_path)\n",
    "\n",
    "        # Persist model and index\n",
    "        self.model.save(self._word2vec_model_path)\n",
    "\n",
    "        save_json(self._doc_id_path, self.documents)\n",
    "\n",
    "    def load_index(self):\n",
    "        self.documents = {int(k): v for k, v in load_json(self._doc_id_path).items()}\n",
    "        self.model = Word2Vec.load(self._word2vec_model_path)\n",
    "        vector_size = self.model.vector_size\n",
    "        self.annoy_index = AnnoyIndex(vector_size, 'angular')\n",
    "        self.annoy_index.load(self._annoy_index_path)\n",
    "\n",
    "    def find(self, query: str, top_k: int = 10) -> list:\n",
    "        query_words = self._tokenize(query)\n",
    "        query_vectors = [\n",
    "            self.model.wv[word] \n",
    "            for word in query_words \n",
    "            if word in self.model.wv\n",
    "        ]\n",
    "        \n",
    "        if not query_vectors:\n",
    "            return []\n",
    "            \n",
    "        # Average word vectors for query embedding\n",
    "        query_embedding = np.mean(query_vectors, axis=0)\n",
    "        \n",
    "        # Find similar documents using Annoy\n",
    "        doc_ids, distances = self.annoy_index.get_nns_by_vector(\n",
    "            query_embedding, \n",
    "            top_k, \n",
    "            include_distances=True\n",
    "        )\n",
    "        \n",
    "        # Convert angular distances to cosine similarities\n",
    "        results = []\n",
    "        for doc_id, distance in zip(doc_ids, distances):\n",
    "            cosine_sim = 1 - (distance ** 2) / 2  # Convert angular distance to cosine\n",
    "            results.append((doc_id, cosine_sim))\n",
    "        \n",
    "        return [\n",
    "            (self.documents[doc_id], round_float(score, 5))\n",
    "            for doc_id, score in sorted(results, key=lambda x: -x[1])\n",
    "        ]\n",
    "\n",
    "\n",
    "indexer = Word2VecIndexer()\n",
    "results = indexer.find('''url''', top_k=10)\n",
    "for doc, score in results:\n",
    "    print(f\"Score: {score}\\tFile: {doc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
