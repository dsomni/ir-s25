{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0297586",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T10:55:28.290589Z",
     "start_time": "2025-05-14T10:55:28.257542Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "220586d53129cbbd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T10:56:21.064614Z",
     "start_time": "2025-05-14T10:56:17.792802Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences 5719\n",
      "Embeddings 8\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "k must be less than or equal to the number of training points",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 172\u001b[0m\n\u001b[0;32m    170\u001b[0m \u001b[38;5;66;03m# Query the tree\u001b[39;00m\n\u001b[0;32m    171\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msin\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 172\u001b[0m results, distances \u001b[38;5;241m=\u001b[39m \u001b[43mball_tree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_distances\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    174\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQuery: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquery\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTop 3 results:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[3], line 118\u001b[0m, in \u001b[0;36mBERTBallTree.query\u001b[1;34m(self, query_text, k, return_distances)\u001b[0m\n\u001b[0;32m    113\u001b[0m query_embedding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedder\u001b[38;5;241m.\u001b[39mtext_to_embedding(\n\u001b[0;32m    114\u001b[0m     query_text, pooling\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m'\u001b[39m, normalize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    115\u001b[0m )\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    117\u001b[0m \u001b[38;5;66;03m# Query the tree\u001b[39;00m\n\u001b[1;32m--> 118\u001b[0m distances, indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_embedding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;66;03m# Get the corresponding texts\u001b[39;00m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;28mprint\u001b[39m(indices[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[1;32msklearn\\\\neighbors\\\\_binary_tree.pxi:1180\u001b[0m, in \u001b[0;36msklearn.neighbors._ball_tree.BinaryTree64.query\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: k must be less than or equal to the number of training points"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "from sklearn.neighbors import BallTree\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "from utils import from_current_file\n",
    "\n",
    "\n",
    "class BERTEmbedder:\n",
    "    def __init__(self, model_name=\"sentence-transformers/stsb-bert-base\", device=None):\n",
    "        self.device = device or \"cpu\"\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModel.from_pretrained(model_name).to(self.device)\n",
    "        self.model.eval()\n",
    "\n",
    "        self.do_lower_case = getattr(self.tokenizer, \"do_lower_case\", False)\n",
    "\n",
    "    def text_to_embedding(self, texts, pooling=\"mean\", normalize=False):\n",
    "        is_single = isinstance(texts, str)\n",
    "        texts = [texts] if is_single else texts\n",
    "\n",
    "        inputs = self.tokenizer(\n",
    "            texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=128\n",
    "        ).to(self.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "\n",
    "        if pooling == \"mean\":\n",
    "            mask = inputs[\"attention_mask\"].unsqueeze(-1)\n",
    "            embeddings = (outputs.last_hidden_state * mask).sum(1) / mask.sum(1).clamp(\n",
    "                min=1e-9\n",
    "            )\n",
    "        elif pooling == \"cls\":\n",
    "            embeddings = outputs.last_hidden_state[:, 0, :]\n",
    "        else:\n",
    "            raise ValueError(\"Invalid pooling method\")\n",
    "\n",
    "        if normalize:\n",
    "            embeddings = torch.nn.functional.normalize(embeddings, p=2, dim=1)\n",
    "\n",
    "        return embeddings.cpu().numpy()[0] if is_single else embeddings.cpu().numpy()\n",
    "\n",
    "\n",
    "class BERTBallTree:\n",
    "    def __init__(\n",
    "        self,\n",
    "        index_dir: str = \"../data/embedding_directory\",\n",
    "        documents_dir: str = \"../data/scrapped/class_data_function__1_1\",\n",
    "        top_similar: int = 10,\n",
    "        force: bool = False,\n",
    "        embedder=None,\n",
    "        metric=\"euclidean\",\n",
    "        leaf_size=400,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the BERT Ball Tree.\n",
    "\n",
    "        Args:\n",
    "            embedder: Pre-initialized BERTEmbedder instance\n",
    "            metric: Distance metric for BallTree ('euclidean', 'cosine', etc.)\n",
    "            leaf_size: Affects the speed of queries and memory usage\n",
    "        \"\"\"\n",
    "        self._index_dir = from_current_file(index_dir)\n",
    "        self._documents_dir = from_current_file(documents_dir)\n",
    "        self.top_similar = top_similar\n",
    "        self.embedder = embedder or BERTEmbedder()\n",
    "        self.metric = metric\n",
    "        self.leaf_size = leaf_size\n",
    "        self.tree = None\n",
    "        self.texts = None\n",
    "        self.documents: dict[int, str] = {}\n",
    "\n",
    "    def build_tree(self):\n",
    "        \"\"\"\n",
    "        Build the Ball Tree from a list of texts.\n",
    "\n",
    "        Args:\n",
    "            texts: List of strings to index\n",
    "        \"\"\"\n",
    "        sentences = []\n",
    "        for document_id, filename in enumerate(os.listdir(self._documents_dir)):\n",
    "            if filename.endswith(\".txt\"):\n",
    "                with open(\n",
    "                    os.path.join(self._documents_dir, filename), \"r\", encoding=\"utf-8\"\n",
    "                ) as f:\n",
    "                    text = f.read()\n",
    "                    self.documents[document_id] = filename[:-4]\n",
    "                    sentences.append(text)\n",
    "        print(\"Sentences\", len(sentences))\n",
    "        embeddings = self.embedder.text_to_embedding(\n",
    "            texts, pooling=\"mean\", normalize=True\n",
    "        )\n",
    "        print(\"Embeddings\", len(embeddings))\n",
    "        self.tree = BallTree(embeddings, metric=self.metric, leaf_size=self.leaf_size)\n",
    "\n",
    "    def query(self, query_text, k=5, return_distances=False):\n",
    "        \"\"\"\n",
    "        Query the Ball Tree for nearest neighbors.\n",
    "\n",
    "        Args:\n",
    "            query_text: The query text string\n",
    "            k: Number of nearest neighbors to return\n",
    "            return_distances: Whether to return distances along with results\n",
    "\n",
    "        Returns:\n",
    "            If return_distances is False: list of nearest texts\n",
    "            If return_distances is True: tuple of (texts, distances)\n",
    "        \"\"\"\n",
    "        if self.tree is None:\n",
    "            raise ValueError(\"Ball Tree has not been built yet. Call build_tree() first.\")\n",
    "\n",
    "        # Get embedding for the query text\n",
    "        query_embedding = self.embedder.text_to_embedding(\n",
    "            query_text, pooling=\"mean\", normalize=True\n",
    "        ).reshape(1, -1)\n",
    "\n",
    "        # Query the tree\n",
    "        distances, indices = self.tree.query(query_embedding, k=k)\n",
    "\n",
    "        # Get the corresponding texts\n",
    "        print(indices[0])\n",
    "        results = []\n",
    "        for indice in indices[0]:\n",
    "            results.append(self.documents[indice])\n",
    "\n",
    "        if return_distances:\n",
    "            return results, distances[0]\n",
    "        return results\n",
    "\n",
    "    def save_tree(self, filepath):\n",
    "        \"\"\"Save the Ball Tree and associated data to disk.\"\"\"\n",
    "        import joblib\n",
    "\n",
    "        data = {\n",
    "            \"texts\": self.texts,\n",
    "            \"tree\": self.tree,\n",
    "            \"metric\": self.metric,\n",
    "            \"leaf_size\": self.leaf_size,\n",
    "        }\n",
    "        joblib.dump(data, filepath)\n",
    "\n",
    "    @classmethod\n",
    "    def load_tree(cls, filepath, embedder=None):\n",
    "        \"\"\"Load a saved Ball Tree from disk.\"\"\"\n",
    "        import joblib\n",
    "\n",
    "        data = joblib.load(filepath)\n",
    "        instance = cls(\n",
    "            embedder=embedder, metric=data[\"metric\"], leaf_size=data[\"leaf_size\"]\n",
    "        )\n",
    "        instance.texts = data[\"texts\"]\n",
    "        instance.tree = data[\"tree\"]\n",
    "        return instance\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Sample texts\n",
    "    texts = [\n",
    "        \"The quick brown fox jumps over the lazy dog\",\n",
    "        \"Artificial intelligence is transforming industries\",\n",
    "        \"Python is a popular programming language\",\n",
    "        \"Machine learning requires large amounts of data\",\n",
    "        \"Deep learning models use neural networks\",\n",
    "        \"Natural language processing helps computers understand text\",\n",
    "        \"The weather is nice today\",\n",
    "        \"I enjoy reading books in my free time\",\n",
    "    ]\n",
    "\n",
    "    # Initialize and build the tree\n",
    "    ball_tree = BERTBallTree()\n",
    "    ball_tree.build_tree()\n",
    "\n",
    "    # Query the tree\n",
    "    query = \"sin\"\n",
    "    results, distances = ball_tree.query(query, k=10, return_distances=True)\n",
    "\n",
    "    print(f\"Query: {query}\")\n",
    "    print(\"Top 3 results:\")\n",
    "    for text, dist in zip(results, distances):\n",
    "        print(f\"- {text} (distance: {dist:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11f6a5072022656f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T10:56:21.123432400Z",
     "start_time": "2025-05-14T10:47:10.800301Z"
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'BERTBallTree' object has no attribute 'find'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m indexer \u001b[38;5;241m=\u001b[39m BERTBallTree()\n\u001b[1;32m----> 2\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mindexer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m(\u001b[38;5;124m'''\u001b[39m\u001b[38;5;124murl\u001b[39m\u001b[38;5;124m'''\u001b[39m, top_k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc, score \u001b[38;5;129;01min\u001b[39;00m results:\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mScore: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mscore\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mFile: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdoc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'BERTBallTree' object has no attribute 'find'"
     ]
    }
   ],
   "source": [
    "indexer = BERTBallTree()\n",
    "results = indexer.find(\"\"\"url\"\"\", top_k=10)\n",
    "for doc, score in results:\n",
    "    print(f\"Score: {score}\\tFile: {doc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595327f7b3e82c60",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
