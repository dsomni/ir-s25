{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8f7a0f7d832e57f",
   "metadata": {},
   "source": [
    "# Norvig Spell Corrector\n",
    "https://norvig.com/spell-correct.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ba0cc45b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d22d42c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to D:\\Code\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to D:\\Code\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to D:\\Code\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to D:\\Code\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import string\n",
    "from collections import Counter, defaultdict\n",
    "from pathlib import Path\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from tqdm import tqdm\n",
    "\n",
    "from utils import load_json, save_json\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"punkt_tab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-05T13:58:52.301492Z",
     "start_time": "2025-03-05T13:58:52.295739Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'python: is language'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class NorvigSpellCorrector:\n",
    "    letters = \"abcdefghijklmnopqrstuvwxyz\"\n",
    "    _stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        spell_dir: str = \"../data/spell_directory\",\n",
    "        documents_dir: str = \"../data/scrapped/class_data_function__1_1\",\n",
    "        max_edits: int = 2,\n",
    "        save_distances: bool = False,\n",
    "    ):\n",
    "        self._max_edits = max_edits\n",
    "        self.save_distances = save_distances\n",
    "\n",
    "        self._spell_dir = spell_dir\n",
    "        self._documents_dir = documents_dir\n",
    "\n",
    "        self._counter_path = os.path.join(self._spell_dir, \"counter.json\")\n",
    "        self._settings_path = os.path.join(self._spell_dir, \"settings.json\")\n",
    "\n",
    "        if not os.path.exists(self._spell_dir):\n",
    "            print(\"Spell index is not found, creating new...\")\n",
    "            os.mkdir(path=self._spell_dir)\n",
    "            self.build_index()\n",
    "            print(\"Complete!\")\n",
    "\n",
    "        self.load_index()\n",
    "\n",
    "    def tokenize(self, text: str) -> list[str]:\n",
    "        return word_tokenize(text.lower())\n",
    "\n",
    "    def filter_stopwords(self, words: list[str]) -> list[str]:\n",
    "        return [word for word in words if word not in self._stop_words]\n",
    "\n",
    "    def generate_edits(self, word: str) -> set[str]:\n",
    "        splits = [(word[:i], word[i:]) for i in range(len(word) + 1)]\n",
    "        deletes = [L + R[1:] for L, R in splits if R]\n",
    "        transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R) > 1]\n",
    "        replaces = [L + c + R[1:] for L, R in splits if R for c in self.letters]\n",
    "        inserts = [L + c + R for L, R in splits for c in self.letters]\n",
    "        return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "    def build_index(self):\n",
    "        # Words counter\n",
    "        words_counter = Counter(\n",
    "            self.filter_stopwords(\n",
    "                self.tokenize(\n",
    "                    \" \".join(\n",
    "                        [\n",
    "                            re.sub(\n",
    "                                r\"[^\\x00-\\x7F]+\",\n",
    "                                \" \",\n",
    "                                open(str(path), encoding=\"utf-8\")\n",
    "                                .read()\n",
    "                                .lower()\n",
    "                                .replace(\"\\n\", \" \"),\n",
    "                            )\n",
    "                            for path in Path(self._documents_dir).rglob(\"*.txt\")\n",
    "                        ]\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "        save_json(self._counter_path, words_counter)\n",
    "        settings = {\"total\": words_counter.total(), \"max_edits\": self._max_edits}\n",
    "        save_json(self._settings_path, settings)\n",
    "\n",
    "        if self.save_distances:\n",
    "            edit_dicts = [defaultdict(set) for _ in range(self._max_edits)]\n",
    "            for word in tqdm(words_counter):\n",
    "                edits = {word}\n",
    "                for i in range(self._max_edits):\n",
    "                    temp_edits: set[str] = set()\n",
    "                    for w in edits:\n",
    "                        temp_edits.update(self.generate_edits(w))\n",
    "                        edit_dicts[i][w].add(word)\n",
    "                    edits = temp_edits\n",
    "\n",
    "            for idx in range(self._max_edits):\n",
    "                save_json(\n",
    "                    os.path.join(self._spell_dir, f\"distance{idx + 1}\"), edit_dicts[idx]\n",
    "                )\n",
    "\n",
    "    def load_index(self):\n",
    "        self.settings = load_json(self._settings_path)\n",
    "        if self.save_distances and self.settings[\"max_edits\"] != self._max_edits:\n",
    "            raise RuntimeError(\"'max_edits' does not match!\")\n",
    "        self.total_sum = self.settings[\"total\"]\n",
    "        self.words_counter = Counter(load_json(self._counter_path))\n",
    "\n",
    "        if self.save_distances:\n",
    "            self.distance_dicts = []\n",
    "            for idx in range(self._max_edits):\n",
    "                self.distance_dicts.append(\n",
    "                    load_json(os.path.join(self._spell_dir, f\"distance{idx + 1}\"))\n",
    "                )\n",
    "\n",
    "    def word_probability(self, word: str) -> float:\n",
    "        return self.words_counter[word] / self.total_sum\n",
    "\n",
    "    def filter_known(self, words: list[str] | set[str]) -> set[str]:\n",
    "        return set(w for w in words if w in self.words_counter)\n",
    "\n",
    "    def _word_candidates_saved(self, word: str) -> list[str]:\n",
    "        if len(self.filter_known([word])) > 0:\n",
    "            return [word]\n",
    "        if self.save_distances:\n",
    "            return self._word_candidates_saved(word)\n",
    "        for i in range(self._max_edits):\n",
    "            knowns = self.distance_dicts[i].get(word, None)\n",
    "            if knowns is not None:\n",
    "                return knowns\n",
    "        return [word]\n",
    "\n",
    "    def word_candidates(self, word: str) -> list[str]:\n",
    "        if self.save_distances:\n",
    "            return self._word_candidates_saved(word)\n",
    "\n",
    "        if len(self.filter_known([word])) > 0:\n",
    "            return [word]\n",
    "\n",
    "        edits = {word}\n",
    "        for _ in range(self._max_edits):\n",
    "            current_edits: set[str] = set()\n",
    "            for w in edits:\n",
    "                current_edits.update(self.generate_edits(w))\n",
    "            knowns = self.filter_known(current_edits)\n",
    "            if len(knowns) > 0:\n",
    "                return list(knowns)\n",
    "            edits = current_edits\n",
    "        return [word]\n",
    "\n",
    "    def spell_correction_word(self, word: str) -> str:\n",
    "        return max(self.word_candidates(word), key=self.word_probability)\n",
    "\n",
    "    def spell_correction(self, text: str) -> str:\n",
    "        tokens = self.tokenize(text.lower())\n",
    "        if len(tokens) == 1:\n",
    "            return self.spell_correction_word(tokens[0])\n",
    "        corrected = \"\"\n",
    "        for idx, token in enumerate(tokens):\n",
    "            if token in string.punctuation:\n",
    "                if len(corrected) > 0 and corrected[-1] == \" \":\n",
    "                    corrected = corrected[:-1]\n",
    "                corrected = corrected + token + \" \"\n",
    "                continue\n",
    "            if token in self._stop_words:\n",
    "                corrected = corrected + token + \" \"\n",
    "                continue\n",
    "            corrected = corrected + self.spell_correction_word(token) + \" \"\n",
    "        return corrected.strip()\n",
    "\n",
    "\n",
    "corrector = NorvigSpellCorrector()\n",
    "corrector.spell_correction(\"pithon: is longuoge\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
